{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Iceberg Banking Reconciliation Demo\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "This comprehensive notebook demonstrates advanced Apache Iceberg features for banking reconciliation systems. You will learn:\n",
    "\n",
    "### **Advanced Iceberg Capabilities**\n",
    "- **Schema Evolution**: Adding, removing, and modifying columns without downtime\n",
    "- **Time Travel**: Querying historical data states and audit trails\n",
    "- **Partition Evolution**: Optimizing performance through partition strategy changes\n",
    "- **ACID Transactions**: Ensuring data consistency with atomic operations\n",
    "- **Incremental Processing**: Efficient handling of new data streams\n",
    "\n",
    "This notebook demonstrates the key features of the Apache Iceberg Banking Reconciliation System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's initialize our Spark session with Iceberg configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/bitnami/python/lib/python3.12/site-packages/pip-23.3.2-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --root-user-action=ignore rich boto3 --quiet\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Warehouse directory: /opt/bitnami/spark/warehouse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/22 07:54:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/22 07:54:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/22 07:54:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Stopped existing Spark session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 07:54:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/22 07:54:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Spark session created successfully\n",
      "Spark version: 3.5.6\n",
      "Default catalog: local\n",
      "Warehouse location: file:///opt/bitnami/spark/warehouse\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, abs, lit, expr, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.functions import concat\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.9.2\"\n",
    "\n",
    "# Ù…Ø³ÛŒØ± warehouse\n",
    "warehouse_dir = \"/opt/bitnami/spark/warehouse\"\n",
    "os.makedirs(warehouse_dir, exist_ok=True)\n",
    "print(f\"âœ“ Warehouse directory: {warehouse_dir}\")\n",
    "\n",
    "# ØªÙˆÙ‚Ù Ø³Ø´Ù† Ù‚Ø¨Ù„ÛŒ Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯\n",
    "try:\n",
    "    SparkSession.builder.getOrCreate().stop()\n",
    "    print(\"âœ“ Stopped existing Spark session\")\n",
    "except:\n",
    "    print(\"â„¹ No existing Spark session to stop\")\n",
    "\n",
    "\n",
    "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø³Ù¾Ø§Ø±Ú© Ùˆ Ø¢ÛŒØ³Ø¨Ø±Ú¯\n",
    "config = {\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.catalog.local\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"spark.sql.catalog.local.type\": \"hadoop\",\n",
    "    \"spark.sql.catalog.local.warehouse\": f\"file://{warehouse_dir}\",\n",
    "    \"spark.sql.defaultCatalog\": \"local\",\n",
    "    \"spark.executor.memory\": \"1024m\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION}\",\n",
    "}\n",
    "\n",
    "spark_config = SparkConf().setAppName(\"Banking Reconciliation Demo\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n",
    "\n",
    "print(\"âœ“ Spark session created successfully\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Default catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n",
    "print(f\"Warehouse location: {spark.conf.get('spark.sql.catalog.local.warehouse')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Iceberg Tables\n",
    "\n",
    "Let's explore the Iceberg tables we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  banking|reconciliation_ba...|      false|\n",
      "|  banking|reconciliation_re...|      false|\n",
      "|  banking| source_transactions|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all tables in the banking namespace\n",
    "spark.sql(\"SHOW TABLES IN local.banking\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|      transaction_id|              string|   NULL|\n",
      "|       source_system|              string|   NULL|\n",
      "|    transaction_date|           timestamp|   NULL|\n",
      "|              amount|       decimal(18,2)|   NULL|\n",
      "|          account_id|              string|   NULL|\n",
      "|    transaction_type|              string|   NULL|\n",
      "|        reference_id|              string|   NULL|\n",
      "|              status|              string|   NULL|\n",
      "|             payload|              string|   NULL|\n",
      "|          created_at|           timestamp|   NULL|\n",
      "|processing_timestamp|           timestamp|   NULL|\n",
      "|                    |                    |       |\n",
      "|      # Partitioning|                    |       |\n",
      "|              Part 0|days(transaction_...|       |\n",
      "|              Part 1|       source_system|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the source_transactions table\n",
    "spark.sql(\"DESCRIBE TABLE local.banking.source_transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:==================================================>      (42 + 5) / 47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|  source_system|transaction_count|\n",
      "+---------------+-----------------+\n",
      "| card_processor|             5050|\n",
      "|   core_banking|             5000|\n",
      "|payment_gateway|             5026|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Check the number of transactions by source system\n",
    "spark.sql(\"\"\"\n",
    "    SELECT source_system, COUNT(*) as transaction_count\n",
    "    FROM local.banking.source_transactions\n",
    "    GROUP BY source_system\n",
    "    ORDER BY source_system\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demonstrate Iceberg Features\n",
    "\n",
    "### 3.1 Schema Evolution\n",
    "\n",
    "Let's demonstrate schema evolution by adding a new column to the source_transactions table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column to the source_transactions table\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.banking.source_transactions\n",
    "    ADD COLUMN transaction_category STRING\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|      transaction_id|              string|   NULL|\n",
      "|       source_system|              string|   NULL|\n",
      "|    transaction_date|           timestamp|   NULL|\n",
      "|              amount|       decimal(18,2)|   NULL|\n",
      "|          account_id|              string|   NULL|\n",
      "|    transaction_type|              string|   NULL|\n",
      "|        reference_id|              string|   NULL|\n",
      "|              status|              string|   NULL|\n",
      "|             payload|              string|   NULL|\n",
      "|          created_at|           timestamp|   NULL|\n",
      "|processing_timestamp|           timestamp|   NULL|\n",
      "|transaction_category|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|      # Partitioning|                    |       |\n",
      "|              Part 0|days(transaction_...|       |\n",
      "|              Part 1|       source_system|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the new column was added\n",
    "spark.sql(\"DESCRIBE TABLE local.banking.source_transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update some records with the new column\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE local.banking.source_transactions\n",
    "    SET transaction_category = \n",
    "        CASE \n",
    "            WHEN transaction_type = 'deposit' THEN 'INCOME'\n",
    "            WHEN transaction_type = 'withdrawal' THEN 'EXPENSE'\n",
    "            WHEN transaction_type = 'transfer' THEN 'TRANSFER'\n",
    "            WHEN transaction_type = 'payment' THEN 'EXPENSE'\n",
    "            WHEN transaction_type = 'refund' THEN 'INCOME'\n",
    "            WHEN transaction_type = 'fee' THEN 'FEE'\n",
    "            ELSE 'OTHER'\n",
    "        END\n",
    "    WHERE source_system = 'core_banking'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----+\n",
      "|transaction_type|transaction_category|count|\n",
      "+----------------+--------------------+-----+\n",
      "|         deposit|              INCOME|  826|\n",
      "|             fee|                 FEE|  841|\n",
      "|         payment|             EXPENSE|  825|\n",
      "|          refund|              INCOME|  866|\n",
      "|        transfer|            TRANSFER|  812|\n",
      "|      withdrawal|             EXPENSE|  830|\n",
      "+----------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the data with the new column\n",
    "spark.sql(\"\"\"\n",
    "    SELECT transaction_type, transaction_category, COUNT(*) as count\n",
    "    FROM local.banking.source_transactions\n",
    "    WHERE source_system = 'core_banking'\n",
    "    GROUP BY transaction_type, transaction_category\n",
    "    ORDER BY transaction_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Time Travel\n",
    "\n",
    "Let's demonstrate time travel by querying the table at different points in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-07-22 07:55:...|7845908200839870587|1423606334598951226|overwrite|file:/opt/bitnami...|{spark.app.id -> ...|\n",
      "|2025-07-22 07:52:...|1423606334598951226|9061708065984232342|   append|file:/opt/bitnami...|{spark.app.id -> ...|\n",
      "|2025-07-22 07:52:...|9061708065984232342| 966229618760623534|   append|file:/opt/bitnami...|{spark.app.id -> ...|\n",
      "|2025-07-22 07:52:...| 966229618760623534|               NULL|   append|file:/opt/bitnami...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the current snapshot information\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM local.banking.source_transactions.snapshots\n",
    "    ORDER BY committed_at DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous snapshot timestamp: 2025-07-22 07:52:56.135000\n"
     ]
    }
   ],
   "source": [
    "# Store the timestamp of the snapshot before our update\n",
    "snapshots = spark.sql(\"\"\"\n",
    "    SELECT * FROM local.banking.source_transactions.snapshots\n",
    "    ORDER BY committed_at DESC\n",
    "    LIMIT 2\n",
    "\"\"\").collect()\n",
    "\n",
    "# Get the timestamp of the previous snapshot\n",
    "if len(snapshots) >= 2:\n",
    "    previous_snapshot_timestamp = snapshots[1][\"committed_at\"]\n",
    "    print(f\"Previous snapshot timestamp: {previous_snapshot_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|transaction_type|count|\n",
      "+----------------+-----+\n",
      "|         deposit|  826|\n",
      "|             fee|  841|\n",
      "|         payment|  825|\n",
      "|          refund|  866|\n",
      "|        transfer|  812|\n",
      "|      withdrawal|  830|\n",
      "+----------------+-----+\n",
      "\n",
      "Error (expected): [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `transaction_category` cannot be resolved. Did you mean one of the following? [`transaction_date`, `transaction_type`, `transaction_id`, `created_at`, `account_id`].; line 2 pos 19;\n",
      "'Sort ['transaction_category ASC NULLS FIRST], true\n",
      "+- 'Aggregate ['transaction_category], ['transaction_category, count(1) AS count#341L]\n",
      "   +- Filter (source_system#343 = core_banking)\n",
      "      +- SubqueryAlias local.banking.source_transactions\n",
      "         +- RelationV2[transaction_id#342, source_system#343, transaction_date#344, amount#345, account_id#346, transaction_type#347, reference_id#348, status#349, payload#350, created_at#351, processing_timestamp#352] local.banking.source_transactions local.banking.source_transactions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the table as of the previous snapshot (before adding the new column)\n",
    "if 'previous_snapshot_timestamp' in locals():\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT transaction_type, COUNT(*) as count\n",
    "        FROM local.banking.source_transactions\n",
    "        FOR TIMESTAMP AS OF '{previous_snapshot_timestamp}'\n",
    "        WHERE source_system = 'core_banking'\n",
    "        GROUP BY transaction_type\n",
    "        ORDER BY transaction_type\n",
    "    \"\"\").show()\n",
    "    \n",
    "    # This would fail because the column didn't exist in the previous snapshot\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT transaction_category, COUNT(*) as count\n",
    "            FROM local.banking.source_transactions\n",
    "            FOR TIMESTAMP AS OF '{previous_snapshot_timestamp}'\n",
    "            WHERE source_system = 'core_banking'\n",
    "            GROUP BY transaction_category\n",
    "            ORDER BY transaction_category\n",
    "        \"\"\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error (expected): {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Partition Evolution\n",
    "\n",
    "Let's demonstrate partition evolution by changing the partition spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|           partition|spec_id|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|     last_updated_at|last_updated_snapshot_id|\n",
      "+--------------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "|{2025-07-06, core...|      1|          29|         1|                         5972|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-06-13, paym...|      0|          96|         1|                         8824|                           0|                         0|                           0|                         0|2025-07-13 09:56:...|     1178840682442895382|\n",
      "|{2025-06-28, core...|      1|          37|         1|                         6405|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-06-18, core...|      1|          64|         1|                         7838|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-06-19, core...|      1|          59|         1|                         7569|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-07-13, paym...|      0|          74|         1|                         7949|                           0|                         0|                           0|                         0|2025-07-13 09:56:...|     1178840682442895382|\n",
      "|{2025-06-29, core...|      1|          32|         1|                         6052|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-07-10, card...|      0|         169|         1|                        11856|                           0|                         0|                           0|                         0|2025-07-13 09:56:...|     6929672523092115133|\n",
      "|{2025-06-17, card...|      0|         172|         1|                        12081|                           0|                         0|                           0|                         0|2025-07-13 09:56:...|     6929672523092115133|\n",
      "|{2025-07-06, paym...|      0|         149|         1|                        11156|                           0|                         0|                           0|                         0|2025-07-13 09:56:...|     1178840682442895382|\n",
      "|{2025-07-12, core...|      1|          48|         1|                         6936|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-06-21, core...|      1|          29|         1|                         6011|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-07-11, core...|      1|          52|         1|                         7179|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-06-26, card...|      0|         163|         1|                        11726|                           0|                         0|                           0|                         0|2025-07-13 09:56:...|     6929672523092115133|\n",
      "|{2025-07-12, core...|      1|          27|         1|                         5950|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-07-02, core...|      1|          73|         1|                         8060|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-06-20, core...|      1|          27|         1|                         5871|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-07-03, core...|      1|          68|         1|                         7771|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "|{2025-06-27, paym...|      0|         160|         1|                        11619|                           0|                         0|                           0|                         0|2025-07-13 09:56:...|     1178840682442895382|\n",
      "|{2025-06-13, core...|      1|          15|         1|                         5151|                           0|                         0|                           0|                         0|2025-07-13 12:25:...|     6569966068115906138|\n",
      "+--------------------+-------+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+--------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the current partition spec\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM local.banking.source_transactions.partitions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new partition field\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.banking.source_transactions\n",
    "    ADD PARTITION FIELD transaction_category\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the updated partition spec\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM local.banking.source_transactions.partitions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run a Reconciliation Process\n",
    "\n",
    "Let's run a reconciliation process to match transactions across systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "# Import necessary modules\n",
    "import os\n",
    "\n",
    "# Add the correct path to sys.path\n",
    "sys.path.append('/opt/bitnami/spark/src/main/python')\n",
    "# sys.path.append('/opt/spark')\n",
    "\n",
    "from etl.extractors import TransactionExtractor\n",
    "from etl.transformers import TransactionTransformer\n",
    "from reconciliation.matcher import TransactionMatcher\n",
    "from reconciliation.reporter import ReconciliationReporter\n",
    "from etl.loaders import IcebergLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running reconciliation for batch DEMO-5a45e038\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running reconciliation for batch DEMO-5a45e038\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Source systems: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'core_banking'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'card_processor'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Source systems: \u001b[1m[\u001b[0m\u001b[32m'core_banking'\u001b[0m, \u001b[32m'card_processor'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Date range: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">06</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">13:17:29</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">814239</span> to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">13:17:29</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">814239</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Date range: \u001b[1;36m2025\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m13\u001b[0m \u001b[1;92m13:17:29\u001b[0m.\u001b[1;36m814239\u001b[0m to \u001b[1;36m2025\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m13\u001b[0m \u001b[1;92m13:17:29\u001b[0m.\u001b[1;36m814239\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define reconciliation parameters\n",
    "batch_id = f\"DEMO-{uuid.uuid4().hex[:8]}\"\n",
    "source_systems = ['core_banking', 'card_processor']\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=30)\n",
    "\n",
    "print(f\"Running reconciliation for batch {batch_id}\")\n",
    "print(f\"Source systems: {source_systems}\")\n",
    "print(f\"Date range: {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 batches into reconciliation_batches table\n"
     ]
    }
   ],
   "source": [
    "# Define the schema explicitly\n",
    "# Register reconciliation batch\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, ArrayType, IntegerType\n",
    "\n",
    "\n",
    "batch_schema = StructType([\n",
    "    StructField(\"batch_id\", StringType(), False),\n",
    "    StructField(\"reconciliation_date\", TimestampType(), True),\n",
    "    StructField(\"source_systems\", ArrayType(StringType()), True),\n",
    "    StructField(\"start_date\", TimestampType(), True),\n",
    "    StructField(\"end_date\", TimestampType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"total_transactions\", IntegerType(), True),\n",
    "    StructField(\"matched_count\", IntegerType(), True),\n",
    "    StructField(\"unmatched_count\", IntegerType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"completed_at\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "batch_df = spark.createDataFrame([{\n",
    "    \"batch_id\": batch_id,\n",
    "    \"reconciliation_date\": datetime.now(),\n",
    "    \"source_systems\": source_systems,\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"status\": \"IN_PROGRESS\",\n",
    "    \"total_transactions\": 0,\n",
    "    \"matched_count\": 0,\n",
    "    \"unmatched_count\": 0,\n",
    "    \"created_at\": datetime.now(),\n",
    "    \"completed_at\": None\n",
    "}], schema=batch_schema)\n",
    "\n",
    "loader = IcebergLoader(spark)\n",
    "loader.load_reconciliation_batch(batch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">core_banking: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4981</span> transactions\n",
       "</pre>\n"
      ],
      "text/plain": [
       "core_banking: \u001b[1;36m4981\u001b[0m transactions\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">card_processor: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5030</span> transactions\n",
       "</pre>\n"
      ],
      "text/plain": [
       "card_processor: \u001b[1;36m5030\u001b[0m transactions\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract transactions\n",
    "extractor = TransactionExtractor(spark)\n",
    "transactions_by_source = extractor.extract_transactions_for_reconciliation(\n",
    "    source_systems, start_date, end_date\n",
    ")\n",
    "\n",
    "# Print transaction counts\n",
    "for source, df in transactions_by_source.items():\n",
    "    print(f\"{source}: {df.count()} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform transactions\n",
    "transformer = TransactionTransformer(spark)\n",
    "prepared_transactions = transformer.prepare_for_reconciliation(transactions_by_source)\n",
    "\n",
    "# Get primary and secondary DataFrames\n",
    "primary_source = source_systems[0]\n",
    "secondary_source = source_systems[1]\n",
    "primary_df = prepared_transactions[primary_source]\n",
    "secondary_df = prepared_transactions[secondary_source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Matched: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4873</span> transactions\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Matched: \u001b[1;36m4873\u001b[0m transactions\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Unmatched in core_banking: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">108</span> transactions\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Unmatched in core_banking: \u001b[1;36m108\u001b[0m transactions\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Unmatched in card_processor: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">157</span> transactions\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Unmatched in card_processor: \u001b[1;36m157\u001b[0m transactions\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Match transactions\n",
    "matcher = TransactionMatcher(spark)\n",
    "matched_df, unmatched_primary_df, unmatched_secondary_df = matcher.match_transactions(\n",
    "    primary_df, secondary_df, match_strategy=\"hybrid\"\n",
    ")\n",
    "\n",
    "# Print matching results\n",
    "print(f\"Matched: {matched_df.count()} transactions\")\n",
    "print(f\"Unmatched in {primary_source}: {unmatched_primary_df.count()} transactions\")\n",
    "print(f\"Unmatched in {secondary_source}: {unmatched_secondary_df.count()} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5138 results into reconciliation_results table\n"
     ]
    }
   ],
   "source": [
    "# Create reconciliation results\n",
    "results_df = matcher.create_reconciliation_results(\n",
    "    batch_id,\n",
    "    matched_df,\n",
    "    unmatched_primary_df,\n",
    "    unmatched_secondary_df,\n",
    "    primary_source,\n",
    "    secondary_source\n",
    ")\n",
    "\n",
    "# Save reconciliation results\n",
    "loader.load_reconciliation_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-----+------------------------+\n",
      "|     batch_id|match_status|count|total_discrepancy_amount|\n",
      "+-------------+------------+-----+------------------------+\n",
      "|DEMO-5a45e038|     MATCHED| 4810|                    0.00|\n",
      "|DEMO-5a45e038|     PARTIAL|   63|                  337.34|\n",
      "|DEMO-5a45e038|   UNMATCHED|  265|              1267091.26|\n",
      "+-------------+------------+-----+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate reports\n",
    "reporter = ReconciliationReporter(spark)\n",
    "summary_report = reporter.generate_summary_report(results_df)\n",
    "discrepancy_report = reporter.generate_discrepancy_report(results_df)\n",
    "\n",
    "# Display summary report\n",
    "summary_report.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------------------+------------------------+------------+----------------+------------------+------------------------+--------------------+\n",
      "|   reconciliation_id|     batch_id|primary_transaction_id|secondary_transaction_id|match_status|discrepancy_type|discrepancy_amount|reconciliation_timestamp|               notes|\n",
      "+--------------------+-------------+----------------------+------------------------+------------+----------------+------------------+------------------------+--------------------+\n",
      "|c241147f-128f-416...|DEMO-5a45e038|       CB-0531DDDA18F2|         CP-CBD78B2DB14D|     PARTIAL|          STATUS|              NULL|    2025-07-13 13:18:...|Partial match bet...|\n",
      "|826d8f32-807d-474...|DEMO-5a45e038|       CB-05B6C2A6B2B1|         CP-A48E95951AF9|     PARTIAL|          STATUS|              NULL|    2025-07-13 13:18:...|Partial match bet...|\n",
      "|5ef66ee8-f21d-4c6...|DEMO-5a45e038|       CB-067A134D2E29|         CP-E3DE2F3FFC4F|     PARTIAL|          STATUS|              NULL|    2025-07-13 13:18:...|Partial match bet...|\n",
      "|7b59152c-6e34-498...|DEMO-5a45e038|       CB-0F64F57FB541|         CP-68898C6E919D|     PARTIAL|          STATUS|              NULL|    2025-07-13 13:18:...|Partial match bet...|\n",
      "|acfb2289-cad7-434...|DEMO-5a45e038|       CB-1420CDE9B5CD|         CP-70022E482A35|     PARTIAL|          STATUS|              NULL|    2025-07-13 13:18:...|Partial match bet...|\n",
      "|05866969-2305-4a0...|DEMO-5a45e038|       CB-1AE04759FB07|         CP-127BFCAA4F69|     PARTIAL|          STATUS|              NULL|    2025-07-13 13:18:...|Partial match bet...|\n",
      "|c1690b41-526a-444...|DEMO-5a45e038|       CB-1D75A6D36390|         CP-5E7BD242B3B9|     PARTIAL|          STATUS|              NULL|    2025-07-13 13:18:...|Partial match bet...|\n",
      "|ebeeb05a-ed6e-4e0...|DEMO-5a45e038|       CB-1DCA890535A8|         CP-E55BDF2705CF|     PARTIAL|          STATUS|              NULL|    2025-07-13 13:18:...|Partial match bet...|\n",
      "|5ea166dc-fe5c-4ca...|DEMO-5a45e038|       CB-32944FBA8CF8|         CP-6A227164A531|     PARTIAL|          AMOUNT|             24.10|    2025-07-13 13:18:...|Partial match bet...|\n",
      "|d173eed0-1d2d-4ac...|DEMO-5a45e038|       CB-344778405257|         CP-EFB4B60D87BD|     PARTIAL|          STATUS|              NULL|    2025-07-13 13:18:...|Partial match bet...|\n",
      "+--------------------+-------------+----------------------+------------------------+------------+----------------+------------------+------------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display discrepancy report (first 10 rows)\n",
    "discrepancy_report.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update batch status\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE local.banking.reconciliation_batches\n",
    "    SET \n",
    "        status = 'COMPLETED',\n",
    "        matched_count = {matched_df.count()},\n",
    "        unmatched_count = {unmatched_primary_df.count() + unmatched_secondary_df.count()},\n",
    "        total_transactions = {matched_df.count() + unmatched_primary_df.count() + unmatched_secondary_df.count()},\n",
    "        completed_at = CURRENT_TIMESTAMP()\n",
    "    WHERE batch_id = '{batch_id}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demonstrate ACID Transactions\n",
    "\n",
    "Let's demonstrate ACID transactions by performing a multi-statement transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Operations completed successfully.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Operations completed successfully.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "import uuid\n",
    "\n",
    "try:\n",
    "    # 1. Build a DataFrame for updates\n",
    "    updates = spark.sql(\"\"\"\n",
    "        SELECT transaction_id AS id\n",
    "        FROM local.banking.source_transactions\n",
    "        WHERE status = 'pending' AND source_system = 'core_banking'\n",
    "    \"\"\").withColumn(\"status\", lit(\"completed\"))\n",
    "\n",
    "    updates.createOrReplaceTempView(\"updates\")\n",
    "\n",
    "    # 2. Perform MERGE INTO for atomic update\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO local.banking.source_transactions AS t\n",
    "        USING updates AS u\n",
    "          ON t.transaction_id = u.id\n",
    "        WHEN MATCHED THEN\n",
    "          UPDATE SET t.status = u.status\n",
    "    \"\"\")\n",
    "\n",
    "    # 3. Insert a new batch row\n",
    "    new_batch_id = f\"ACID-{uuid.uuid4().hex[:8]}\"\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO local.banking.reconciliation_batches VALUES (\n",
    "            '{new_batch_id}',\n",
    "            CURRENT_TIMESTAMP(),\n",
    "            ARRAY('core_banking', 'payment_gateway'),\n",
    "            TIMESTAMP('{start_date}'),\n",
    "            TIMESTAMP('{end_date}'),\n",
    "            'PENDING',\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            CURRENT_TIMESTAMP(),\n",
    "            NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    print(\"Operations completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during operations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+--------------------+-------+------------------+-------------+---------------+--------------------+------------+\n",
      "|     batch_id| reconciliation_date|      source_systems|          start_date|            end_date| status|total_transactions|matched_count|unmatched_count|          created_at|completed_at|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+-------+------------------+-------------+---------------+--------------------+------------+\n",
      "|ACID-b3048f2e|2025-07-13 13:30:...|[core_banking, pa...|2025-06-13 13:17:...|2025-07-13 13:17:...|PENDING|                 0|            0|              0|2025-07-13 13:30:...|        NULL|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+-------+------------------+-------------+---------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the changes\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM local.banking.reconciliation_batches\n",
    "    WHERE batch_id = '{new_batch_id}'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demonstrate Incremental Processing\n",
    "\n",
    "Let's demonstrate incremental processing by processing only new transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Latest snapshot timestamp: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">13:30:40</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">990000</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Latest snapshot timestamp: \u001b[1;36m2025\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m13\u001b[0m \u001b[1;92m13:30:40\u001b[0m.\u001b[1;36m990000\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the latest snapshot timestamp\n",
    "latest_snapshot = spark.sql(\"\"\"\n",
    "    SELECT * FROM local.banking.source_transactions.snapshots\n",
    "    ORDER BY committed_at DESC\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "latest_timestamp = latest_snapshot[\"committed_at\"]\n",
    "print(f\"Latest snapshot timestamp: {latest_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-------+--------------------+--------------+-------------+---------+--------------------+--------------------+--------------+----------------+\n",
      "| account_id|              amount|          created_at|payload|processing_timestamp|  reference_id|source_system|   status|transaction_category|    transaction_date|transaction_id|transaction_type|\n",
      "+-----------+--------------------+--------------------+-------+--------------------+--------------+-------------+---------+--------------------+--------------------+--------------+----------------+\n",
      "|ACC32063425|703.3700000000000...|2025-07-13 13:33:...|     {}|2025-07-13 13:33:...|REF-4246999770| core_banking|completed|                 NEW|2025-07-13 13:33:...|  NEW-b678ff98|         deposit|\n",
      "|ACC57490967|826.9200000000000...|2025-07-13 13:33:...|     {}|2025-07-13 13:33:...|REF-8898056994| core_banking|completed|                 NEW|2025-07-13 13:33:...|  NEW-d367651a|        transfer|\n",
      "|ACC61505689|619.0200000000000...|2025-07-13 13:33:...|     {}|2025-07-13 13:33:...|REF-2847976194| core_banking|completed|                 NEW|2025-07-13 13:33:...|  NEW-92737577|      withdrawal|\n",
      "|ACC49465221|268.8400000000000...|2025-07-13 13:33:...|     {}|2025-07-13 13:33:...|REF-2624998429| core_banking|  pending|                 NEW|2025-07-13 13:33:...|  NEW-6d0a4205|        transfer|\n",
      "|ACC21944868|665.9200000000000...|2025-07-13 13:33:...|     {}|2025-07-13 13:33:...|REF-4010619024| core_banking|  pending|                 NEW|2025-07-13 13:33:...|  NEW-c507d596|         payment|\n",
      "+-----------+--------------------+--------------------+-------+--------------------+--------------+-------------+---------+--------------------+--------------------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create some new transactions\n",
    "import pandas as pd\n",
    "import random\n",
    "from decimal import Decimal\n",
    "\n",
    "# Generate 10 new transactions\n",
    "new_transactions = []\n",
    "for i in range(10):\n",
    "    tx_id = f\"NEW-{uuid.uuid4().hex[:8]}\"\n",
    "    source_system = \"core_banking\"\n",
    "    tx_date = datetime.now()\n",
    "    amount = Decimal(random.uniform(100, 1000)).quantize(Decimal('0.01'))\n",
    "    account_id = f\"ACC{random.randint(10000000, 99999999)}\"\n",
    "    tx_type = random.choice([\"deposit\", \"withdrawal\", \"transfer\", \"payment\"])\n",
    "    ref_id = f\"REF-{random.randint(1000000000, 9999999999)}\"\n",
    "    status = random.choice([\"completed\", \"pending\"])\n",
    "    \n",
    "    new_transactions.append({\n",
    "        \"transaction_id\": tx_id,\n",
    "        \"source_system\": source_system,\n",
    "        \"transaction_date\": tx_date,\n",
    "        \"amount\": amount,\n",
    "        \"account_id\": account_id,\n",
    "        \"transaction_type\": tx_type,\n",
    "        \"reference_id\": ref_id,\n",
    "        \"status\": status,\n",
    "        \"payload\": \"{}\",\n",
    "        \"created_at\": tx_date,\n",
    "        \"processing_timestamp\": tx_date,\n",
    "        \"transaction_category\": \"NEW\"\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the new transactions\n",
    "new_tx_df = spark.createDataFrame(new_transactions)\n",
    "new_tx_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 transactions into source_transactions table\n",
      "Loaded 10 new transactions incrementally\n"
     ]
    }
   ],
   "source": [
    "# Load the new transactions incrementally\n",
    "loader = IcebergLoader(spark)\n",
    "loader.load_transactions_incrementally(\n",
    "    new_tx_df, \n",
    "    \"core_banking\",\n",
    "    snapshot_time=datetime.now()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+--------------------+------+-----------+----------------+--------------+---------+-------+--------------------+--------------------+--------------------+\n",
      "|transaction_id|source_system|    transaction_date|amount| account_id|transaction_type|  reference_id|   status|payload|          created_at|processing_timestamp|transaction_category|\n",
      "+--------------+-------------+--------------------+------+-----------+----------------+--------------+---------+-------+--------------------+--------------------+--------------------+\n",
      "|  NEW-c35b672a| core_banking|2025-07-13 13:33:...|785.20|ACC92554416|         deposit|REF-6811757290|  pending|     {}|2025-07-13 13:33:...|2025-07-13 13:33:...|                 NEW|\n",
      "|  NEW-c684bc0f| core_banking|2025-07-13 13:33:...|831.84|ACC68619522|         payment|REF-8337618624|  pending|     {}|2025-07-13 13:33:...|2025-07-13 13:33:...|                 NEW|\n",
      "|  NEW-ebcdce89| core_banking|2025-07-13 13:33:...|100.51|ACC86556910|        transfer|REF-2538514282|  pending|     {}|2025-07-13 13:33:...|2025-07-13 13:33:...|                 NEW|\n",
      "|  NEW-37e97a78| core_banking|2025-07-13 13:33:...|872.41|ACC73753850|      withdrawal|REF-7900701632|  pending|     {}|2025-07-13 13:33:...|2025-07-13 13:33:...|                 NEW|\n",
      "|  NEW-a38b066b| core_banking|2025-07-13 13:33:...|422.43|ACC64806919|      withdrawal|REF-9905050397|  pending|     {}|2025-07-13 13:33:...|2025-07-13 13:33:...|                 NEW|\n",
      "|  NEW-c507d596| core_banking|2025-07-13 13:33:...|665.92|ACC21944868|         payment|REF-4010619024|  pending|     {}|2025-07-13 13:33:...|2025-07-13 13:33:...|                 NEW|\n",
      "|  NEW-6d0a4205| core_banking|2025-07-13 13:33:...|268.84|ACC49465221|        transfer|REF-2624998429|  pending|     {}|2025-07-13 13:33:...|2025-07-13 13:33:...|                 NEW|\n",
      "|  NEW-92737577| core_banking|2025-07-13 13:33:...|619.02|ACC61505689|      withdrawal|REF-2847976194|completed|     {}|2025-07-13 13:33:...|2025-07-13 13:33:...|                 NEW|\n",
      "|  NEW-d367651a| core_banking|2025-07-13 13:33:...|826.92|ACC57490967|        transfer|REF-8898056994|completed|     {}|2025-07-13 13:33:...|2025-07-13 13:33:...|                 NEW|\n",
      "|  NEW-b678ff98| core_banking|2025-07-13 13:33:...|703.37|ACC32063425|         deposit|REF-4246999770|completed|     {}|2025-07-13 13:33:...|2025-07-13 13:33:...|                 NEW|\n",
      "+--------------+-------------+--------------------+------+-----------+----------------+--------------+---------+-------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query only the new transactions added since the last snapshot\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM local.banking.source_transactions\n",
    "    WHERE created_at > TIMESTAMP('{latest_timestamp}')\n",
    "    ORDER BY transaction_date DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the key features of the Apache Iceberg Banking Reconciliation System:\n",
    "\n",
    "1. **Schema Evolution**: Adding new columns without rebuilding tables\n",
    "2. **Time Travel**: Querying historical states of the data\n",
    "3. **Partition Evolution**: Changing partition specifications\n",
    "4. **ACID Transactions**: Ensuring consistency during updates\n",
    "5. **Incremental Processing**: Processing only new data\n",
    "6. **Reconciliation Process**: Matching transactions across systems\n",
    "\n",
    "These features make Apache Iceberg an excellent choice for banking reconciliation systems, providing the reliability, flexibility, and performance needed for financial data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
