{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Banking Reconciliation Setup - Phase 1\n",
    "## MinIO S3-backed Iceberg Catalog\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "This notebook demonstrates how to set up Apache Iceberg with a **MinIO S3-compatible object storage catalog** for a banking reconciliation system. You will learn:\n",
    "\n",
    "### **Apache Iceberg Fundamentals**\n",
    "- **Catalog System**: Understanding how Iceberg catalogs work with S3/MinIO\n",
    "- **MinIO Storage**: How Iceberg stores metadata and data in MinIO buckets\n",
    "- **Table Structure**: The file organization created by Iceberg tables in S3\n",
    "- **Partitioning**: How Iceberg handles partitioned tables\n",
    "\n",
    "### **What You'll Build**\n",
    "- A complete banking reconciliation system with three core tables\n",
    "- S3-based storage with MinIO integration\n",
    "- Partitioned tables for efficient querying\n",
    "- Comprehensive audit trail and validation\n",
    "\n",
    "### **Key Concepts Explained**\n",
    "\n",
    "#### **1. Iceberg Catalog with MinIO (S3)**\n",
    "```python\n",
    "spark.sql.catalog.minio.type = \"hadoop\"\n",
    "spark.sql.catalog.minio.warehouse = \"s3a://warehouse/\"\n",
    "```\n",
    "- **Catalog**: A namespace for tables, schemas, and functions\n",
    "- **MinIO**: S3-compatible object storage for all table data and metadata\n",
    "- **Warehouse**: Data and metadata stored in MinIO buckets\n",
    "\n",
    "#### **2. File Structure Created by Iceberg in S3**\n",
    "\n",
    "```\n",
    "s3a://warehouse/banking/table_name/\n",
    "‚îú‚îÄ‚îÄ metadata/ # Table metadata and version history\n",
    "‚îÇ ‚îú‚îÄ‚îÄ v1.metadata.json # Current table schema and properties\n",
    "‚îÇ ‚îú‚îÄ‚îÄ version-hint.text # Points to latest metadata version\n",
    "‚îÇ ‚îî‚îÄ‚îÄ ..crc # Checksums for data integrity\n",
    "‚îú‚îÄ‚îÄ data/ # Actual data files (Parquet format)\n",
    "‚îÇ ‚îî‚îÄ‚îÄ partition_paths/ # Partitioned data organized by partition keys\n",
    "‚îî‚îÄ‚îÄ snapshots/ # Table snapshots for time travel\n",
    "```\n",
    "\n",
    "\n",
    "#### **3. Partitioning Strategy**\n",
    "- **source_transactions**: Partitioned by `days(transaction_date)` and `source_system`\n",
    "- **reconciliation_results**: Partitioned by `days(reconciliation_timestamp)` and `match_status`\n",
    "- **reconciliation_batches**: No partitioning (small lookup table)\n",
    "\n",
    "## Phase 1: Foundation Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "**Purpose**: Import all necessary libraries for Spark, MinIO, and file operations.\n",
    "\n",
    "**Key Libraries**:\n",
    "- `pyspark.sql.SparkSession`: Core Spark functionality\n",
    "- `boto3`: AWS S3/MinIO client for object storage\n",
    "- `os`, `json`, `datetime`: File and data manipulation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Spark Version: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> - Spark Minor Version: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span> - Iceberg Version: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.9</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Spark Version: \u001b[1;36m3.5\u001b[0m.\u001b[1;36m6\u001b[0m - Spark Minor Version: \u001b[1;36m3.5\u001b[0m - Iceberg Version: \u001b[1;36m1.9\u001b[0m.\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "!pip install --root-user-action=ignore rich --quiet\n",
    "from rich import print\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# This CATALOG_URL works for the \"docker compose\" testing and development environment\n",
    "# Change 'lakekeeper' if you are not running on \"docker compose\" (f. ex. 'localhost' if Lakekeeper is running locally).\n",
    "CATALOG_URL = \"http://lakekeeper:8181/catalog\"\n",
    "WAREHOUSE = \"irisa\"\n",
    "\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.9.2\"\n",
    "\n",
    "print(f\"Spark Version: {SPARK_VERSION} - Spark Minor Version: {SPARK_MINOR_VERSION} - Iceberg Version: {ICEBERG_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Stop any existing Spark session\n",
    "\n",
    "**Purpose**: Ensure a clean Spark environment by stopping any existing sessions.\n",
    "\n",
    "**Why This Matters**:\n",
    "- Prevents configuration conflicts\n",
    "- Ensures fresh Iceberg catalog initialization\n",
    "- Clears any cached metadata or connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/21 11:32:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚úì Stopped existing Spark session\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚úì Stopped existing Spark session\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop any existing Spark session\n",
    "try:\n",
    "    SparkSession.builder.getOrCreate().stop()\n",
    "    print(\"‚úì Stopped existing Spark session\")\n",
    "except:\n",
    "    print(\"‚Ñπ No existing Spark session to stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Spark Session with Iceberg + MinIO (S3) Configuration\n",
    "\n",
    "**Purpose**: Initialize Spark with Apache Iceberg extensions and MinIO S3-compatible catalog configuration.\n",
    "\n",
    "### **Configuration Breakdown**:\n",
    "\n",
    "#### **Iceberg Extensions**\n",
    "```python\n",
    "spark.sql.extensions = \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "```\n",
    "- Enables Iceberg-specific SQL commands (CREATE TABLE, MERGE, etc.)\n",
    "- Provides time travel, schema evolution, and partition evolution capabilities\n",
    "\n",
    "#### **Catalog Configuration**\n",
    "```python\n",
    "spark.sql.catalog.minio = \"org.apache.iceberg.spark.SparkCatalog\"\n",
    "spark.sql.catalog.minio.type = \"hadoop\"\n",
    "spark.sql.catalog.minio.warehouse = \"s3a://warehouse/\"\n",
    "spark.hadoop.fs.s3a.endpoint = \"http://minio:9000\"\n",
    "spark.hadoop.fs.s3a.access.key = \"minio\"\n",
    "spark.hadoop.fs.s3a.secret.key = \"minio123\"\n",
    "spark.hadoop.fs.s3a.path.style.access = \"true\"\n",
    "spark.hadoop.fs.s3a.impl = \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n",
    "```\n",
    "- **minio**: Our custom catalog name for S3/MinIO\n",
    "- **warehouse**: S3 bucket path for table data and metadata\n",
    "\n",
    "#### **Default Catalog**\n",
    "- You will use `minio` as the catalog name in all SQL operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o157.sql.\n: org.apache.iceberg.exceptions.RESTException: Unable to process: Warehouse irisa not found\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:248)\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:212)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:215)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:299)\n\tat org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.fetchConfig(RESTSessionCatalog.java:1021)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:202)\n\tat org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:82)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.SetCatalogAndNamespace.mapChildren(v2Commands.scala:941)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m     spark_config = spark_config.set(k, v)\n\u001b[32m     17\u001b[39m spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUSE lakekeeper\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úì Spark session created successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpark version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark.version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o157.sql.\n: org.apache.iceberg.exceptions.RESTException: Unable to process: Warehouse irisa not found\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:248)\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:212)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:215)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:299)\n\tat org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.fetchConfig(RESTSessionCatalog.java:1021)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:202)\n\tat org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:82)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.SetCatalogAndNamespace.mapChildren(v2Commands.scala:941)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "# Create warehouse directory if it doesn't exist\n",
    "config = {\n",
    "    f\"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.lakekeeper.type\": \"rest\",\n",
    "    f\"spark.sql.catalog.lakekeeper.uri\": CATALOG_URL,\n",
    "    f\"spark.sql.catalog.lakekeeper.warehouse\": WAREHOUSE,\n",
    "    f\"spark.sql.catalog.lakekeeper.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n",
    "    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION}\",\n",
    "}\n",
    "\n",
    "spark_config = SparkConf().setMaster('spark://spark-master:7077').setAppName(\"Iceberg-REST-Cluster-Banking-Sample-Phase1\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n",
    "\n",
    "spark.sql(\"USE lakekeeper\")\n",
    "print(\"‚úì Spark session created successfully\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Default catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize MinIO Client and Check Health\n",
    "\n",
    "**Purpose**: Set up MinIO (S3-compatible object storage) for future data operations.\n",
    "\n",
    "### **MinIO Configuration**:\n",
    "- **Endpoint**: `http://minio:9000` (Docker service name)\n",
    "- **Credentials**: `minio`/`minio123` (default development credentials)\n",
    "- **Region**: `us-east-1` (standard region)\n",
    "- **Signature Version**: `s3v4` (AWS Signature Version 4)\n",
    "\n",
    "### **Health Check Strategy**:\n",
    "- Retry logic with exponential backoff\n",
    "- Graceful handling of connection failures\n",
    "- Detailed logging for troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "# Initialize MinIO client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://minio:9000',\n",
    "    aws_access_key_id='minio',\n",
    "    aws_secret_access_key='minio123',\n",
    "    config=Config(signature_version='s3v4'),\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "# Initialize MinIO client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://minio:9000',\n",
    "    aws_access_key_id='minio-root-user',\n",
    "    aws_secret_access_key='minio-root-password',\n",
    "    config=Config(signature_version='s3v4'),\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(\"‚úì MinIO client initialized\")\n",
    "print(f\"Endpoint: http://minio:9000\")\n",
    "print(f\"Signature Version: s3v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check MinIO health with retries\n",
    "print(\"Checking MinIO health...\")\n",
    "max_attempts = 30\n",
    "attempt = 1\n",
    "\n",
    "while attempt <= max_attempts:\n",
    "    try:\n",
    "        # Try to list buckets to check connectivity\n",
    "        buckets = s3_client.list_buckets()\n",
    "        print(f\"‚úì MinIO is ready! Found {len(buckets['Buckets'])} existing buckets\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Waiting for MinIO... (attempt {attempt}/{max_attempts}): {str(e)}\")\n",
    "        if attempt >= max_attempts:\n",
    "            print(\"‚ö† MinIO failed to start within the expected time. Continuing anyway...\")\n",
    "            break\n",
    "        time.sleep(2)\n",
    "        attempt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize MinIO Buckets\n",
    "\n",
    "**Purpose**: Create the required S3 buckets for different stages of data processing.\n",
    "\n",
    "### **Bucket Strategy**:\n",
    "\n",
    "| Bucket | Purpose | Data Type |\n",
    "|--------|---------|-----------|\n",
    "| `warehouse` | Iceberg table data and metadata | Structured data |\n",
    "| `raw-data` | Original source files | CSV, JSON, etc. |\n",
    "| `stage-data` | Intermediate processing data | Transformed data |\n",
    "| `reconciled-data` | Final reconciliation results | Processed results |\n",
    "\n",
    "### **Benefits of This Structure**:\n",
    "- **Data Lineage**: Clear separation of data stages\n",
    "- **Cost Optimization**: Different retention policies per bucket\n",
    "- **Security**: Granular access controls per bucket\n",
    "- **Performance**: Optimized storage for each data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing buckets\n",
    "try:\n",
    "    existing_buckets = [bucket['Name'] for bucket in s3_client.list_buckets()['Buckets']]\n",
    "    print(f\"Existing buckets: {existing_buckets}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing buckets: {str(e)}\")\n",
    "    existing_buckets = []\n",
    "\n",
    "# Define required buckets with their purposes\n",
    "bucket_purposes = {\n",
    "    'warehouse': 'Iceberg table data and metadata',\n",
    "    'raw-data': 'Original source files (CSV, JSON)',\n",
    "    'stage-data': 'Intermediate processing data',\n",
    "    'reconciled-data': 'Final reconciliation results'\n",
    "}\n",
    "\n",
    "# Create buckets if they don't exist\n",
    "created_buckets = []\n",
    "for bucket, purpose in bucket_purposes.items():\n",
    "    if bucket not in existing_buckets:\n",
    "        try:\n",
    "            s3_client.create_bucket(Bucket=bucket)\n",
    "            created_buckets.append(bucket)\n",
    "            print(f\"‚úì Created bucket: {bucket} ({purpose})\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error creating bucket {bucket}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"‚Ñπ Bucket already exists: {bucket} ({purpose})\")\n",
    "\n",
    "print(f\"\\nüìä Bucket Summary:\")\n",
    "print(f\"- Total buckets: {len(existing_buckets) + len(created_buckets)}\")\n",
    "print(f\"- Newly created: {len(created_buckets)}\")\n",
    "print(f\"- Already existed: {len(existing_buckets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Banking Namespace\n",
    "\n",
    "**Purpose**: Create a logical namespace to organize related tables in MinIO.\n",
    "\n",
    "### **Namespace Benefits**:\n",
    "- **Organization**: Groups related tables together\n",
    "- **Access Control**: Apply permissions at namespace level\n",
    "- **Naming**: Avoid table name conflicts\n",
    "- **Discovery**: Easier to find related tables\n",
    "\n",
    "### **S3 Impact**:\n",
    "The namespace creates a directory structure in your MinIO bucket:\n",
    "```\n",
    "s3a://warehouse/banking/\n",
    "‚îú‚îÄ‚îÄ source_transactions/\n",
    "‚îú‚îÄ‚îÄ reconciliation_results/\n",
    "‚îî‚îÄ‚îÄ reconciliation_batches/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the banking namespace\n",
    "try:\n",
    "    spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakekeeper.banking\")\n",
    "    print(\"‚úì Created namespace: lakekeeper.banking\")\n",
    "    print(f\"Namespace location: lakekeeper/banking\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error creating namespace: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6b: Verify Namespace Creation\n",
    "\n",
    "**Purpose**: Confirm that the namespace directory structure exists in your MinIO bucket.\n",
    "\n",
    "> Note: You can use MinIO Console (http://localhost:9001) or `mc` CLI to browse the S3 structure:\n",
    "> - `s3a://warehouse/banking/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Iceberg Tables in MinIO\n",
    "\n",
    "**Purpose**: Create the core tables for the banking reconciliation system in the MinIO S3 bucket.\n",
    "\n",
    "### **Table Design Strategy**:\n",
    "\n",
    "#### **1. source_transactions**\n",
    "- **Purpose**: Store all transaction data from different source systems\n",
    "- **Partitioning**: By `days(transaction_date)` and `source_system`\n",
    "- **Benefits**: Efficient querying by date range and source\n",
    "- **Schema**: Comprehensive transaction metadata\n",
    "\n",
    "#### **2. reconciliation_results**\n",
    "- **Purpose**: Store reconciliation outcomes and discrepancies\n",
    "- **Partitioning**: By `days(reconciliation_timestamp)` and `match_status`\n",
    "- **Benefits**: Easy analysis of reconciliation performance\n",
    "- **Schema**: Detailed discrepancy tracking\n",
    "\n",
    "#### **3. reconciliation_batches**\n",
    "- **Purpose**: Track reconciliation batch metadata\n",
    "- **Partitioning**: None (small lookup table)\n",
    "- **Benefits**: Batch-level monitoring and reporting\n",
    "- **Schema**: Batch execution metadata\n",
    "\n",
    "> All tables will be created under the `minio.banking` namespace and stored in your MinIO S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create source_transactions table\n",
    "print(\"Creating source_transactions table...\")\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS lakekeeper.banking.source_transactions (\n",
    "      transaction_id STRING,\n",
    "      source_system STRING,\n",
    "      transaction_date TIMESTAMP,\n",
    "      amount DECIMAL(18,2),\n",
    "      account_id STRING,\n",
    "      transaction_type STRING,\n",
    "      reference_id STRING,\n",
    "      status STRING,\n",
    "      payload STRING,\n",
    "      created_at TIMESTAMP,\n",
    "      processing_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(transaction_date), source_system)\n",
    "    \"\"\")\n",
    "    print(\"‚úì Created table: lakekeeper.banking.source_transactions\")\n",
    "    print(\"  - Partitioned by: days(transaction_date), source_system\")\n",
    "    print(\"  - Purpose: Store all transaction data from different sources\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error creating source_transactions table: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reconciliation_results table\n",
    "print(\"Creating reconciliation_results table...\")\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS lakekeeper.banking.reconciliation_results (\n",
    "      reconciliation_id STRING,\n",
    "      batch_id STRING,\n",
    "      primary_transaction_id STRING,\n",
    "      secondary_transaction_id STRING,\n",
    "      match_status STRING,\n",
    "      discrepancy_type STRING,\n",
    "      discrepancy_amount DECIMAL(18,2),\n",
    "      reconciliation_timestamp TIMESTAMP,\n",
    "      notes STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(reconciliation_timestamp), match_status)\n",
    "    \"\"\")\n",
    "    print(\"‚úì Created table: lakekeeper.banking.reconciliation_results\")\n",
    "    print(\"  - Partitioned by: days(reconciliation_timestamp), match_status\")\n",
    "    print(\"  - Purpose: Store reconciliation outcomes and discrepancies\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error creating reconciliation_results table: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reconciliation_batches table\n",
    "print(\"Creating reconciliation_batches table...\")\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS lakekeeper.banking.reconciliation_batches (\n",
    "      batch_id STRING,\n",
    "      reconciliation_date TIMESTAMP,\n",
    "      source_systems ARRAY<STRING>,\n",
    "      start_date TIMESTAMP,\n",
    "      end_date TIMESTAMP,\n",
    "      status STRING,\n",
    "      total_transactions BIGINT,\n",
    "      matched_count BIGINT,\n",
    "      unmatched_count BIGINT,\n",
    "      created_at TIMESTAMP,\n",
    "      completed_at TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "    \"\"\")\n",
    "    print(\"‚úì Created table: minio.banking.reconciliation_batches\")\n",
    "    print(\"  - Partitioned by: None (small lookup table)\")\n",
    "    print(\"  - Purpose: Track reconciliation batch metadata\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error creating reconciliation_batches table: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verify Tables and Audit Setup\n",
    "\n",
    "**Purpose**: Validate that all tables were created correctly in MinIO and examine the Iceberg file structure.\n",
    "\n",
    "### **What We'll Verify**:\n",
    "1. **Table Existence**: Confirm all tables are created in the `minio.banking` namespace\n",
    "2. **File Structure**: Examine Iceberg metadata and data directories in S3\n",
    "3. **Schema Validation**: Check table schemas are correct\n",
    "4. **Accessibility**: Test basic queries on each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables to verify\n",
    "print(\"üìã Verifying created tables...\")\n",
    "tables_df = spark.sql(\"SHOW TABLES IN lakekeeper.banking\")\n",
    "tables_df.show()\n",
    "\n",
    "# Count tables\n",
    "table_count = tables_df.count()\n",
    "print(f\"\\nüìä Table Summary:\")\n",
    "print(f\"- Total tables in banking namespace: {table_count}\")\n",
    "print(f\"- Expected tables: 3\")\n",
    "print(f\"- Status: {'‚úì PASS' if table_count >= 3 else '‚úó FAIL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a simple query on each table\n",
    "print(\"üîç Testing table accessibility...\")\n",
    "\n",
    "tables_to_test = [\n",
    "    'lakekeeper.banking.source_transactions',\n",
    "    'lakekeeper.banking.reconciliation_results',\n",
    "    'lakekeeper.banking.reconciliation_batches'\n",
    "]\n",
    "\n",
    "for table in tables_to_test:\n",
    "    try:\n",
    "        result = spark.sql(f\"SELECT COUNT(*) as count FROM {table}\")\n",
    "        count = result.collect()[0]['count']\n",
    "        print(f\"‚úì {table}: {count} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {table}: Error - {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Examine Iceberg File Structure in MinIO\n",
    "\n",
    "**Purpose**: Understand how Iceberg organizes files and metadata in your MinIO S3 bucket.\n",
    "\n",
    "### **Iceberg File Organization in S3**:\n",
    "\n",
    "Each Iceberg table creates this structure in your MinIO bucket:\n",
    "```\n",
    "s3a://warehouse/banking/table_name/\n",
    "‚îú‚îÄ‚îÄ metadata/              # Table metadata and version history\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ v1.metadata.json  # Current table schema and properties\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ version-hint.text # Points to latest metadata version\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ .*.crc           # Checksums for data integrity\n",
    "‚îú‚îÄ‚îÄ data/                # Actual data files (Parquet format)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ partition_paths/ # Partitioned data organized by partition keys\n",
    "‚îî‚îÄ‚îÄ snapshots/           # Table snapshots for time travel\n",
    "```\n",
    "> Use the MinIO Console (http://localhost:9001) or mc CLI to browse and inspect these files.\n",
    "\n",
    "### **Key Files Explained**:\n",
    "- **v1.metadata.json**: Contains table schema, partitioning, and properties\n",
    "- **version-hint.text**: Points to the current metadata version\n",
    "- **.crc files**: Checksums to ensure data integrity\n",
    "- **data/**: Contains actual Parquet files with the data\n",
    "- **snapshots/**: Enables time travel and rollback capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'warehouse'\n",
    "namespace_prefix = 'banking/'\n",
    "\n",
    "def list_s3_prefix(prefix, title):\n",
    "    print(f\"\\nüîç {title}\")\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            print(obj['Key'])\n",
    "    else:\n",
    "        print(\"No objects found.\")\n",
    "\n",
    "print(\"üìÅ Examining Iceberg file structure...\")\n",
    "\n",
    "# Banking namespace structure\n",
    "list_s3_prefix(namespace_prefix, \"Banking namespace structure\")\n",
    "\n",
    "# source_transactions table structure\n",
    "list_s3_prefix(namespace_prefix + 'source_transactions/', \"source_transactions table structure\")\n",
    "\n",
    "# source_transactions metadata\n",
    "list_s3_prefix(namespace_prefix + 'source_transactions/metadata/', \"source_transactions metadata\")\n",
    "\n",
    "# reconciliation_results table structure\n",
    "list_s3_prefix(namespace_prefix + 'reconciliation_results/', \"reconciliation_results table structure\")\n",
    "\n",
    "# reconciliation_batches table structure\n",
    "list_s3_prefix(namespace_prefix + 'reconciliation_batches/', \"reconciliation_batches table structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the metadata file to understand table schema and properties\n",
    "bucket_name = 'warehouse'\n",
    "metadata_prefix = 'banking/source_transactions/metadata/'\n",
    "\n",
    "def read_s3_object(key, title):\n",
    "    print(f\"\\nüîç {title}\")\n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "        content = obj['Body'].read().decode('utf-8')\n",
    "        print(content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {key}: {e}\")\n",
    "\n",
    "print(\"üìÑ Examining table metadata...\")\n",
    "\n",
    "# Read v1.metadata.json\n",
    "read_s3_object(metadata_prefix + 'v1.metadata.json', \"source_transactions metadata content\")\n",
    "\n",
    "# Read version-hint.text\n",
    "read_s3_object(metadata_prefix + 'version-hint.text', \"version hint file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Phase 1 Summary and Next Steps\n",
    "\n",
    "**Purpose**: Provide a comprehensive summary of what was accomplished and prepare for the next phase.\n",
    "\n",
    "### **What We've Accomplished**:\n",
    "\n",
    "#### **‚úÖ Infrastructure Setup**\n",
    "- Spark session with Iceberg extensions\n",
    "- Local catalog with Hive metastore\n",
    "- MinIO object storage buckets\n",
    "\n",
    "#### **‚úÖ Data Architecture**\n",
    "- Banking namespace for organization\n",
    "- Three core tables with proper schemas\n",
    "- Partitioning strategy for performance\n",
    "\n",
    "#### **‚úÖ File Structure**\n",
    "- Iceberg metadata and data directories\n",
    "- Proper table organization\n",
    "- Version control and checksums\n",
    "\n",
    "### **Next Phase Preview**:\n",
    "- **Data Generation**: Create sample transaction data\n",
    "- **Data Ingestion**: Load data into Iceberg tables\n",
    "- **Reconciliation Logic**: Implement matching algorithms\n",
    "- **Testing**: Validate the complete system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate setup summary\n",
    "print(\"\\nüìö Learning Summary:\")\n",
    "print(\"‚úÖ Understanding of Iceberg local catalog with Hive metastore\")\n",
    "print(\"‚úÖ Knowledge of Iceberg file structure and metadata organization\")\n",
    "print(\"‚úÖ Experience with table partitioning and schema design\")\n",
    "print(\"‚úÖ Familiarity with MinIO object storage integration\")\n",
    "print(\"\\nüöÄ Next: Run Phase 2 notebook for data generation and population.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
