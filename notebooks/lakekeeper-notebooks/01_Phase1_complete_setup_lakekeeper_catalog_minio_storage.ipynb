{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Banking Reconciliation Setup - Phase 1\n",
    "## MinIO S3-backed Iceberg Catalog\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "This notebook demonstrates how to set up Apache Iceberg with a **MinIO S3-compatible object storage catalog** for a banking reconciliation system. You will learn:\n",
    "\n",
    "### **Apache Iceberg Fundamentals**\n",
    "- **Catalog System**: Understanding how Iceberg catalogs work with S3/MinIO\n",
    "- **MinIO Storage**: How Iceberg stores metadata and data in MinIO buckets\n",
    "- **Table Structure**: The file organization created by Iceberg tables in S3\n",
    "- **Partitioning**: How Iceberg handles partitioned tables\n",
    "\n",
    "### **What You'll Build**\n",
    "- A complete banking reconciliation system with three core tables\n",
    "- S3-based storage with MinIO integration\n",
    "- Partitioned tables for efficient querying\n",
    "- Comprehensive audit trail and validation\n",
    "\n",
    "### **Key Concepts Explained**\n",
    "\n",
    "#### **1. Iceberg Catalog with MinIO (S3)**\n",
    "```python\n",
    "spark.sql.catalog.minio.type = \"hadoop\"\n",
    "spark.sql.catalog.minio.warehouse = \"s3a://warehouse/\"\n",
    "```\n",
    "- **Catalog**: A namespace for tables, schemas, and functions\n",
    "- **MinIO**: S3-compatible object storage for all table data and metadata\n",
    "- **Warehouse**: Data and metadata stored in MinIO buckets\n",
    "\n",
    "#### **2. File Structure Created by Iceberg in S3**\n",
    "\n",
    "```\n",
    "s3a://warehouse/banking/table_name/\n",
    "├── metadata/ # Table metadata and version history\n",
    "│ ├── v1.metadata.json # Current table schema and properties\n",
    "│ ├── version-hint.text # Points to latest metadata version\n",
    "│ └── ..crc # Checksums for data integrity\n",
    "├── data/ # Actual data files (Parquet format)\n",
    "│ └── partition_paths/ # Partitioned data organized by partition keys\n",
    "└── snapshots/ # Table snapshots for time travel\n",
    "```\n",
    "\n",
    "\n",
    "#### **3. Partitioning Strategy**\n",
    "- **source_transactions**: Partitioned by `days(transaction_date)` and `source_system`\n",
    "- **reconciliation_results**: Partitioned by `days(reconciliation_timestamp)` and `match_status`\n",
    "- **reconciliation_batches**: No partitioning (small lookup table)\n",
    "\n",
    "## Phase 1: Foundation Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "**Purpose**: Import all necessary libraries for Spark, MinIO, and file operations.\n",
    "\n",
    "**Key Libraries**:\n",
    "- `pyspark.sql.SparkSession`: Core Spark functionality\n",
    "- `boto3`: AWS S3/MinIO client for object storage\n",
    "- `os`, `json`, `datetime`: File and data manipulation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Spark Version: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span> - Spark Minor Version: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span> - Iceberg Version: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.9</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Spark Version: \u001b[1;36m3.5\u001b[0m.\u001b[1;36m6\u001b[0m - Spark Minor Version: \u001b[1;36m3.5\u001b[0m - Iceberg Version: \u001b[1;36m1.9\u001b[0m.\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "!pip install --root-user-action=ignore rich --quiet\n",
    "from rich import print\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# This CATALOG_URL works for the \"docker compose\" testing and development environment\n",
    "# Change 'lakekeeper' if you are not running on \"docker compose\" (f. ex. 'localhost' if Lakekeeper is running locally).\n",
    "CATALOG_URL = \"http://lakekeeper:8181/catalog\"\n",
    "WAREHOUSE = \"irisa\"\n",
    "\n",
    "SPARK_VERSION = pyspark.__version__\n",
    "SPARK_MINOR_VERSION = '.'.join(SPARK_VERSION.split('.')[:2])\n",
    "ICEBERG_VERSION = \"1.9.2\"\n",
    "\n",
    "print(f\"Spark Version: {SPARK_VERSION} - Spark Minor Version: {SPARK_MINOR_VERSION} - Iceberg Version: {ICEBERG_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Stop any existing Spark session\n",
    "\n",
    "**Purpose**: Ensure a clean Spark environment by stopping any existing sessions.\n",
    "\n",
    "**Why This Matters**:\n",
    "- Prevents configuration conflicts\n",
    "- Ensures fresh Iceberg catalog initialization\n",
    "- Clears any cached metadata or connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/21 11:32:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✓ Stopped existing Spark session\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✓ Stopped existing Spark session\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop any existing Spark session\n",
    "try:\n",
    "    SparkSession.builder.getOrCreate().stop()\n",
    "    print(\"✓ Stopped existing Spark session\")\n",
    "except:\n",
    "    print(\"ℹ No existing Spark session to stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Spark Session with Iceberg + MinIO (S3) Configuration\n",
    "\n",
    "**Purpose**: Initialize Spark with Apache Iceberg extensions and MinIO S3-compatible catalog configuration.\n",
    "\n",
    "### **Configuration Breakdown**:\n",
    "\n",
    "#### **Iceberg Extensions**\n",
    "```python\n",
    "spark.sql.extensions = \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "```\n",
    "- Enables Iceberg-specific SQL commands (CREATE TABLE, MERGE, etc.)\n",
    "- Provides time travel, schema evolution, and partition evolution capabilities\n",
    "\n",
    "#### **Catalog Configuration**\n",
    "```python\n",
    "spark.sql.catalog.minio = \"org.apache.iceberg.spark.SparkCatalog\"\n",
    "spark.sql.catalog.minio.type = \"hadoop\"\n",
    "spark.sql.catalog.minio.warehouse = \"s3a://warehouse/\"\n",
    "spark.hadoop.fs.s3a.endpoint = \"http://minio:9000\"\n",
    "spark.hadoop.fs.s3a.access.key = \"minio\"\n",
    "spark.hadoop.fs.s3a.secret.key = \"minio123\"\n",
    "spark.hadoop.fs.s3a.path.style.access = \"true\"\n",
    "spark.hadoop.fs.s3a.impl = \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n",
    "```\n",
    "- **minio**: Our custom catalog name for S3/MinIO\n",
    "- **warehouse**: S3 bucket path for table data and metadata\n",
    "\n",
    "#### **Default Catalog**\n",
    "- You will use `minio` as the catalog name in all SQL operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o157.sql.\n: org.apache.iceberg.exceptions.RESTException: Unable to process: Warehouse irisa not found\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:248)\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:212)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:215)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:299)\n\tat org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.fetchConfig(RESTSessionCatalog.java:1021)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:202)\n\tat org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:82)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.SetCatalogAndNamespace.mapChildren(v2Commands.scala:941)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m     spark_config = spark_config.set(k, v)\n\u001b[32m     17\u001b[39m spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUSE lakekeeper\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Spark session created successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpark version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark.version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/lib/py4j.zip/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o157.sql.\n: org.apache.iceberg.exceptions.RESTException: Unable to process: Warehouse irisa not found\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:248)\n\tat org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:212)\n\tat org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:215)\n\tat org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:299)\n\tat org.apache.iceberg.rest.BaseHTTPClient.get(BaseHTTPClient.java:77)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.fetchConfig(RESTSessionCatalog.java:1021)\n\tat org.apache.iceberg.rest.RESTSessionCatalog.initialize(RESTSessionCatalog.java:202)\n\tat org.apache.iceberg.rest.RESTCatalog.initialize(RESTCatalog.java:82)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\n\tat org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:153)\n\tat org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:752)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)\n\tat org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.SetCatalogAndNamespace.mapChildren(v2Commands.scala:941)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "# Create warehouse directory if it doesn't exist\n",
    "config = {\n",
    "    f\"spark.sql.catalog.lakekeeper\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.lakekeeper.type\": \"rest\",\n",
    "    f\"spark.sql.catalog.lakekeeper.uri\": CATALOG_URL,\n",
    "    f\"spark.sql.catalog.lakekeeper.warehouse\": WAREHOUSE,\n",
    "    f\"spark.sql.catalog.lakekeeper.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"spark.sql.extensions\": \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    \"spark.sql.defaultCatalog\": \"lakekeeper\",\n",
    "    \"spark.jars.packages\": f\"org.apache.iceberg:iceberg-spark-runtime-{SPARK_MINOR_VERSION}_2.12:{ICEBERG_VERSION},org.apache.iceberg:iceberg-aws-bundle:{ICEBERG_VERSION}\",\n",
    "}\n",
    "\n",
    "spark_config = SparkConf().setMaster('spark://spark-master:7077').setAppName(\"Iceberg-REST-Cluster-Banking-Sample-Phase1\")\n",
    "for k, v in config.items():\n",
    "    spark_config = spark_config.set(k, v)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_config).getOrCreate()\n",
    "\n",
    "spark.sql(\"USE lakekeeper\")\n",
    "print(\"✓ Spark session created successfully\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Default catalog: {spark.conf.get('spark.sql.defaultCatalog')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize MinIO Client and Check Health\n",
    "\n",
    "**Purpose**: Set up MinIO (S3-compatible object storage) for future data operations.\n",
    "\n",
    "### **MinIO Configuration**:\n",
    "- **Endpoint**: `http://minio:9000` (Docker service name)\n",
    "- **Credentials**: `minio`/`minio123` (default development credentials)\n",
    "- **Region**: `us-east-1` (standard region)\n",
    "- **Signature Version**: `s3v4` (AWS Signature Version 4)\n",
    "\n",
    "### **Health Check Strategy**:\n",
    "- Retry logic with exponential backoff\n",
    "- Graceful handling of connection failures\n",
    "- Detailed logging for troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "# Initialize MinIO client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://minio:9000',\n",
    "    aws_access_key_id='minio',\n",
    "    aws_secret_access_key='minio123',\n",
    "    config=Config(signature_version='s3v4'),\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "# Initialize MinIO client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://minio:9000',\n",
    "    aws_access_key_id='minio-root-user',\n",
    "    aws_secret_access_key='minio-root-password',\n",
    "    config=Config(signature_version='s3v4'),\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(\"✓ MinIO client initialized\")\n",
    "print(f\"Endpoint: http://minio:9000\")\n",
    "print(f\"Signature Version: s3v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check MinIO health with retries\n",
    "print(\"Checking MinIO health...\")\n",
    "max_attempts = 30\n",
    "attempt = 1\n",
    "\n",
    "while attempt <= max_attempts:\n",
    "    try:\n",
    "        # Try to list buckets to check connectivity\n",
    "        buckets = s3_client.list_buckets()\n",
    "        print(f\"✓ MinIO is ready! Found {len(buckets['Buckets'])} existing buckets\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Waiting for MinIO... (attempt {attempt}/{max_attempts}): {str(e)}\")\n",
    "        if attempt >= max_attempts:\n",
    "            print(\"⚠ MinIO failed to start within the expected time. Continuing anyway...\")\n",
    "            break\n",
    "        time.sleep(2)\n",
    "        attempt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize MinIO Buckets\n",
    "\n",
    "**Purpose**: Create the required S3 buckets for different stages of data processing.\n",
    "\n",
    "### **Bucket Strategy**:\n",
    "\n",
    "| Bucket | Purpose | Data Type |\n",
    "|--------|---------|-----------|\n",
    "| `warehouse` | Iceberg table data and metadata | Structured data |\n",
    "| `raw-data` | Original source files | CSV, JSON, etc. |\n",
    "| `stage-data` | Intermediate processing data | Transformed data |\n",
    "| `reconciled-data` | Final reconciliation results | Processed results |\n",
    "\n",
    "### **Benefits of This Structure**:\n",
    "- **Data Lineage**: Clear separation of data stages\n",
    "- **Cost Optimization**: Different retention policies per bucket\n",
    "- **Security**: Granular access controls per bucket\n",
    "- **Performance**: Optimized storage for each data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing buckets\n",
    "try:\n",
    "    existing_buckets = [bucket['Name'] for bucket in s3_client.list_buckets()['Buckets']]\n",
    "    print(f\"Existing buckets: {existing_buckets}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing buckets: {str(e)}\")\n",
    "    existing_buckets = []\n",
    "\n",
    "# Define required buckets with their purposes\n",
    "bucket_purposes = {\n",
    "    'warehouse': 'Iceberg table data and metadata',\n",
    "    'raw-data': 'Original source files (CSV, JSON)',\n",
    "    'stage-data': 'Intermediate processing data',\n",
    "    'reconciled-data': 'Final reconciliation results'\n",
    "}\n",
    "\n",
    "# Create buckets if they don't exist\n",
    "created_buckets = []\n",
    "for bucket, purpose in bucket_purposes.items():\n",
    "    if bucket not in existing_buckets:\n",
    "        try:\n",
    "            s3_client.create_bucket(Bucket=bucket)\n",
    "            created_buckets.append(bucket)\n",
    "            print(f\"✓ Created bucket: {bucket} ({purpose})\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error creating bucket {bucket}: {str(e)}\")\n",
    "    else:\n",
    "        print(f\"ℹ Bucket already exists: {bucket} ({purpose})\")\n",
    "\n",
    "print(f\"\\n📊 Bucket Summary:\")\n",
    "print(f\"- Total buckets: {len(existing_buckets) + len(created_buckets)}\")\n",
    "print(f\"- Newly created: {len(created_buckets)}\")\n",
    "print(f\"- Already existed: {len(existing_buckets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Banking Namespace\n",
    "\n",
    "**Purpose**: Create a logical namespace to organize related tables in MinIO.\n",
    "\n",
    "### **Namespace Benefits**:\n",
    "- **Organization**: Groups related tables together\n",
    "- **Access Control**: Apply permissions at namespace level\n",
    "- **Naming**: Avoid table name conflicts\n",
    "- **Discovery**: Easier to find related tables\n",
    "\n",
    "### **S3 Impact**:\n",
    "The namespace creates a directory structure in your MinIO bucket:\n",
    "```\n",
    "s3a://warehouse/banking/\n",
    "├── source_transactions/\n",
    "├── reconciliation_results/\n",
    "└── reconciliation_batches/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the banking namespace\n",
    "try:\n",
    "    spark.sql(\"CREATE NAMESPACE IF NOT EXISTS lakekeeper.banking\")\n",
    "    print(\"✓ Created namespace: lakekeeper.banking\")\n",
    "    print(f\"Namespace location: lakekeeper/banking\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating namespace: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6b: Verify Namespace Creation\n",
    "\n",
    "**Purpose**: Confirm that the namespace directory structure exists in your MinIO bucket.\n",
    "\n",
    "> Note: You can use MinIO Console (http://localhost:9001) or `mc` CLI to browse the S3 structure:\n",
    "> - `s3a://warehouse/banking/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Iceberg Tables in MinIO\n",
    "\n",
    "**Purpose**: Create the core tables for the banking reconciliation system in the MinIO S3 bucket.\n",
    "\n",
    "### **Table Design Strategy**:\n",
    "\n",
    "#### **1. source_transactions**\n",
    "- **Purpose**: Store all transaction data from different source systems\n",
    "- **Partitioning**: By `days(transaction_date)` and `source_system`\n",
    "- **Benefits**: Efficient querying by date range and source\n",
    "- **Schema**: Comprehensive transaction metadata\n",
    "\n",
    "#### **2. reconciliation_results**\n",
    "- **Purpose**: Store reconciliation outcomes and discrepancies\n",
    "- **Partitioning**: By `days(reconciliation_timestamp)` and `match_status`\n",
    "- **Benefits**: Easy analysis of reconciliation performance\n",
    "- **Schema**: Detailed discrepancy tracking\n",
    "\n",
    "#### **3. reconciliation_batches**\n",
    "- **Purpose**: Track reconciliation batch metadata\n",
    "- **Partitioning**: None (small lookup table)\n",
    "- **Benefits**: Batch-level monitoring and reporting\n",
    "- **Schema**: Batch execution metadata\n",
    "\n",
    "> All tables will be created under the `minio.banking` namespace and stored in your MinIO S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create source_transactions table\n",
    "print(\"Creating source_transactions table...\")\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS lakekeeper.banking.source_transactions (\n",
    "      transaction_id STRING,\n",
    "      source_system STRING,\n",
    "      transaction_date TIMESTAMP,\n",
    "      amount DECIMAL(18,2),\n",
    "      account_id STRING,\n",
    "      transaction_type STRING,\n",
    "      reference_id STRING,\n",
    "      status STRING,\n",
    "      payload STRING,\n",
    "      created_at TIMESTAMP,\n",
    "      processing_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(transaction_date), source_system)\n",
    "    \"\"\")\n",
    "    print(\"✓ Created table: lakekeeper.banking.source_transactions\")\n",
    "    print(\"  - Partitioned by: days(transaction_date), source_system\")\n",
    "    print(\"  - Purpose: Store all transaction data from different sources\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating source_transactions table: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reconciliation_results table\n",
    "print(\"Creating reconciliation_results table...\")\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS lakekeeper.banking.reconciliation_results (\n",
    "      reconciliation_id STRING,\n",
    "      batch_id STRING,\n",
    "      primary_transaction_id STRING,\n",
    "      secondary_transaction_id STRING,\n",
    "      match_status STRING,\n",
    "      discrepancy_type STRING,\n",
    "      discrepancy_amount DECIMAL(18,2),\n",
    "      reconciliation_timestamp TIMESTAMP,\n",
    "      notes STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(reconciliation_timestamp), match_status)\n",
    "    \"\"\")\n",
    "    print(\"✓ Created table: lakekeeper.banking.reconciliation_results\")\n",
    "    print(\"  - Partitioned by: days(reconciliation_timestamp), match_status\")\n",
    "    print(\"  - Purpose: Store reconciliation outcomes and discrepancies\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating reconciliation_results table: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reconciliation_batches table\n",
    "print(\"Creating reconciliation_batches table...\")\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS lakekeeper.banking.reconciliation_batches (\n",
    "      batch_id STRING,\n",
    "      reconciliation_date TIMESTAMP,\n",
    "      source_systems ARRAY<STRING>,\n",
    "      start_date TIMESTAMP,\n",
    "      end_date TIMESTAMP,\n",
    "      status STRING,\n",
    "      total_transactions BIGINT,\n",
    "      matched_count BIGINT,\n",
    "      unmatched_count BIGINT,\n",
    "      created_at TIMESTAMP,\n",
    "      completed_at TIMESTAMP\n",
    "    )\n",
    "    USING iceberg\n",
    "    \"\"\")\n",
    "    print(\"✓ Created table: minio.banking.reconciliation_batches\")\n",
    "    print(\"  - Partitioned by: None (small lookup table)\")\n",
    "    print(\"  - Purpose: Track reconciliation batch metadata\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating reconciliation_batches table: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verify Tables and Audit Setup\n",
    "\n",
    "**Purpose**: Validate that all tables were created correctly in MinIO and examine the Iceberg file structure.\n",
    "\n",
    "### **What We'll Verify**:\n",
    "1. **Table Existence**: Confirm all tables are created in the `minio.banking` namespace\n",
    "2. **File Structure**: Examine Iceberg metadata and data directories in S3\n",
    "3. **Schema Validation**: Check table schemas are correct\n",
    "4. **Accessibility**: Test basic queries on each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables to verify\n",
    "print(\"📋 Verifying created tables...\")\n",
    "tables_df = spark.sql(\"SHOW TABLES IN lakekeeper.banking\")\n",
    "tables_df.show()\n",
    "\n",
    "# Count tables\n",
    "table_count = tables_df.count()\n",
    "print(f\"\\n📊 Table Summary:\")\n",
    "print(f\"- Total tables in banking namespace: {table_count}\")\n",
    "print(f\"- Expected tables: 3\")\n",
    "print(f\"- Status: {'✓ PASS' if table_count >= 3 else '✗ FAIL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a simple query on each table\n",
    "print(\"🔍 Testing table accessibility...\")\n",
    "\n",
    "tables_to_test = [\n",
    "    'lakekeeper.banking.source_transactions',\n",
    "    'lakekeeper.banking.reconciliation_results',\n",
    "    'lakekeeper.banking.reconciliation_batches'\n",
    "]\n",
    "\n",
    "for table in tables_to_test:\n",
    "    try:\n",
    "        result = spark.sql(f\"SELECT COUNT(*) as count FROM {table}\")\n",
    "        count = result.collect()[0]['count']\n",
    "        print(f\"✓ {table}: {count} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {table}: Error - {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Examine Iceberg File Structure in MinIO\n",
    "\n",
    "**Purpose**: Understand how Iceberg organizes files and metadata in your MinIO S3 bucket.\n",
    "\n",
    "### **Iceberg File Organization in S3**:\n",
    "\n",
    "Each Iceberg table creates this structure in your MinIO bucket:\n",
    "```\n",
    "s3a://warehouse/banking/table_name/\n",
    "├── metadata/              # Table metadata and version history\n",
    "│   ├── v1.metadata.json  # Current table schema and properties\n",
    "│   ├── version-hint.text # Points to latest metadata version\n",
    "│   └── .*.crc           # Checksums for data integrity\n",
    "├── data/                # Actual data files (Parquet format)\n",
    "│   └── partition_paths/ # Partitioned data organized by partition keys\n",
    "└── snapshots/           # Table snapshots for time travel\n",
    "```\n",
    "> Use the MinIO Console (http://localhost:9001) or mc CLI to browse and inspect these files.\n",
    "\n",
    "### **Key Files Explained**:\n",
    "- **v1.metadata.json**: Contains table schema, partitioning, and properties\n",
    "- **version-hint.text**: Points to the current metadata version\n",
    "- **.crc files**: Checksums to ensure data integrity\n",
    "- **data/**: Contains actual Parquet files with the data\n",
    "- **snapshots/**: Enables time travel and rollback capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'warehouse'\n",
    "namespace_prefix = 'banking/'\n",
    "\n",
    "def list_s3_prefix(prefix, title):\n",
    "    print(f\"\\n🔍 {title}\")\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter='/')\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            print(obj['Key'])\n",
    "    else:\n",
    "        print(\"No objects found.\")\n",
    "\n",
    "print(\"📁 Examining Iceberg file structure...\")\n",
    "\n",
    "# Banking namespace structure\n",
    "list_s3_prefix(namespace_prefix, \"Banking namespace structure\")\n",
    "\n",
    "# source_transactions table structure\n",
    "list_s3_prefix(namespace_prefix + 'source_transactions/', \"source_transactions table structure\")\n",
    "\n",
    "# source_transactions metadata\n",
    "list_s3_prefix(namespace_prefix + 'source_transactions/metadata/', \"source_transactions metadata\")\n",
    "\n",
    "# reconciliation_results table structure\n",
    "list_s3_prefix(namespace_prefix + 'reconciliation_results/', \"reconciliation_results table structure\")\n",
    "\n",
    "# reconciliation_batches table structure\n",
    "list_s3_prefix(namespace_prefix + 'reconciliation_batches/', \"reconciliation_batches table structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the metadata file to understand table schema and properties\n",
    "bucket_name = 'warehouse'\n",
    "metadata_prefix = 'banking/source_transactions/metadata/'\n",
    "\n",
    "def read_s3_object(key, title):\n",
    "    print(f\"\\n🔍 {title}\")\n",
    "    try:\n",
    "        obj = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "        content = obj['Body'].read().decode('utf-8')\n",
    "        print(content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {key}: {e}\")\n",
    "\n",
    "print(\"📄 Examining table metadata...\")\n",
    "\n",
    "# Read v1.metadata.json\n",
    "read_s3_object(metadata_prefix + 'v1.metadata.json', \"source_transactions metadata content\")\n",
    "\n",
    "# Read version-hint.text\n",
    "read_s3_object(metadata_prefix + 'version-hint.text', \"version hint file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Phase 1 Summary and Next Steps\n",
    "\n",
    "**Purpose**: Provide a comprehensive summary of what was accomplished and prepare for the next phase.\n",
    "\n",
    "### **What We've Accomplished**:\n",
    "\n",
    "#### **✅ Infrastructure Setup**\n",
    "- Spark session with Iceberg extensions\n",
    "- Local catalog with Hive metastore\n",
    "- MinIO object storage buckets\n",
    "\n",
    "#### **✅ Data Architecture**\n",
    "- Banking namespace for organization\n",
    "- Three core tables with proper schemas\n",
    "- Partitioning strategy for performance\n",
    "\n",
    "#### **✅ File Structure**\n",
    "- Iceberg metadata and data directories\n",
    "- Proper table organization\n",
    "- Version control and checksums\n",
    "\n",
    "### **Next Phase Preview**:\n",
    "- **Data Generation**: Create sample transaction data\n",
    "- **Data Ingestion**: Load data into Iceberg tables\n",
    "- **Reconciliation Logic**: Implement matching algorithms\n",
    "- **Testing**: Validate the complete system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate setup summary\n",
    "print(\"\\n📚 Learning Summary:\")\n",
    "print(\"✅ Understanding of Iceberg local catalog with Hive metastore\")\n",
    "print(\"✅ Knowledge of Iceberg file structure and metadata organization\")\n",
    "print(\"✅ Experience with table partitioning and schema design\")\n",
    "print(\"✅ Familiarity with MinIO object storage integration\")\n",
    "print(\"\\n🚀 Next: Run Phase 2 notebook for data generation and population.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
