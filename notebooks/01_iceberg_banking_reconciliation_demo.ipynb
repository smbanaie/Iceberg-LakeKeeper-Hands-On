{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Iceberg Banking Reconciliation Demo\n",
    "\n",
    "This notebook demonstrates the key features of the Apache Iceberg Banking Reconciliation System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's initialize our Spark session with Iceberg configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/06 14:29:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, when, lit\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create Spark session with Iceberg configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Banking Reconciliation Demo\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"s3a://warehouse/\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minio\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minio123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Iceberg Tables\n",
    "\n",
    "Let's explore the Iceberg tables we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/06 12:28:07 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "java.net.URISyntaxException: Relative path in absolute URI: s3a://warehousebanking",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# List all tables in the banking namespace\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSHOW TABLES IN local.banking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: java.net.URISyntaxException: Relative path in absolute URI: s3a://warehousebanking"
     ]
    }
   ],
   "source": [
    "# List all tables in the banking namespace\n",
    "spark.sql(\"SHOW TABLES IN local.banking\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the source_transactions table\n",
    "spark.sql(\"DESCRIBE TABLE local.banking.source_transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of transactions by source system\n",
    "spark.sql(\"\"\"\n",
    "    SELECT source_system, COUNT(*) as transaction_count\n",
    "    FROM local.banking.source_transactions\n",
    "    GROUP BY source_system\n",
    "    ORDER BY source_system\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demonstrate Iceberg Features\n",
    "\n",
    "### 3.1 Schema Evolution\n",
    "\n",
    "Let's demonstrate schema evolution by adding a new column to the source_transactions table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to the source_transactions table\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.banking.source_transactions\n",
    "    ADD COLUMN transaction_category STRING\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the new column was added\n",
    "spark.sql(\"DESCRIBE TABLE local.banking.source_transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update some records with the new column\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE local.banking.source_transactions\n",
    "    SET transaction_category = \n",
    "        CASE \n",
    "            WHEN transaction_type = 'deposit' THEN 'INCOME'\n",
    "            WHEN transaction_type = 'withdrawal' THEN 'EXPENSE'\n",
    "            WHEN transaction_type = 'transfer' THEN 'TRANSFER'\n",
    "            WHEN transaction_type = 'payment' THEN 'EXPENSE'\n",
    "            WHEN transaction_type = 'refund' THEN 'INCOME'\n",
    "            WHEN transaction_type = 'fee' THEN 'FEE'\n",
    "            ELSE 'OTHER'\n",
    "        END\n",
    "    WHERE source_system = 'core_banking'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data with the new column\n",
    "spark.sql(\"\"\"\n",
    "    SELECT transaction_type, transaction_category, COUNT(*) as count\n",
    "    FROM local.banking.source_transactions\n",
    "    WHERE source_system = 'core_banking'\n",
    "    GROUP BY transaction_type, transaction_category\n",
    "    ORDER BY transaction_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Time Travel\n",
    "\n",
    "Let's demonstrate time travel by querying the table at different points in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current snapshot information\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM local.banking.source_transactions.snapshots\n",
    "    ORDER BY committed_at DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the timestamp of the snapshot before our update\n",
    "snapshots = spark.sql(\"\"\"\n",
    "    SELECT * FROM local.banking.source_transactions.snapshots\n",
    "    ORDER BY committed_at DESC\n",
    "    LIMIT 2\n",
    "\"\"\").collect()\n",
    "\n",
    "# Get the timestamp of the previous snapshot\n",
    "if len(snapshots) >= 2:\n",
    "    previous_snapshot_timestamp = snapshots[1][\"committed_at\"]\n",
    "    print(f\"Previous snapshot timestamp: {previous_snapshot_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the table as of the previous snapshot (before adding the new column)\n",
    "if 'previous_snapshot_timestamp' in locals():\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT transaction_type, COUNT(*) as count\n",
    "        FROM local.banking.source_transactions\n",
    "        FOR TIMESTAMP AS OF '{previous_snapshot_timestamp}'\n",
    "        WHERE source_system = 'core_banking'\n",
    "        GROUP BY transaction_type\n",
    "        ORDER BY transaction_type\n",
    "    \"\"\").show()\n",
    "    \n",
    "    # This would fail because the column didn't exist in the previous snapshot\n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT transaction_category, COUNT(*) as count\n",
    "            FROM local.banking.source_transactions\n",
    "            FOR TIMESTAMP AS OF '{previous_snapshot_timestamp}'\n",
    "            WHERE source_system = 'core_banking'\n",
    "            GROUP BY transaction_category\n",
    "            ORDER BY transaction_category\n",
    "        \"\"\").show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error (expected): {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Partition Evolution\n",
    "\n",
    "Let's demonstrate partition evolution by changing the partition spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current partition spec\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM local.banking.source_transactions.partitions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new partition field\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.banking.source_transactions\n",
    "    ADD PARTITION FIELD transaction_category\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the updated partition spec\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM local.banking.source_transactions.partitions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run a Reconciliation Process\n",
    "\n",
    "Let's run a reconciliation process to match transactions across systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "sys.path.append('/opt/spark')\n",
    "\n",
    "from src.main.python.etl.extractors import TransactionExtractor\n",
    "from src.main.python.etl.transformers import TransactionTransformer\n",
    "from src.main.python.reconciliation.matcher import TransactionMatcher\n",
    "from src.main.python.reconciliation.reporter import ReconciliationReporter\n",
    "from src.main.python.etl.loaders import IcebergLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reconciliation parameters\n",
    "batch_id = f\"DEMO-{uuid.uuid4().hex[:8]}\"\n",
    "source_systems = ['core_banking', 'card_processor']\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=30)\n",
    "\n",
    "print(f\"Running reconciliation for batch {batch_id}\")\n",
    "print(f\"Source systems: {source_systems}\")\n",
    "print(f\"Date range: {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register reconciliation batch\n",
    "batch_df = spark.createDataFrame([{\n",
    "    \"batch_id\": batch_id,\n",
    "    \"reconciliation_date\": datetime.now(),\n",
    "    \"source_systems\": source_systems,\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"status\": \"IN_PROGRESS\",\n",
    "    \"total_transactions\": 0,\n",
    "    \"matched_count\": 0,\n",
    "    \"unmatched_count\": 0,\n",
    "    \"created_at\": datetime.now(),\n",
    "    \"completed_at\": None\n",
    "}])\n",
    "\n",
    "loader = IcebergLoader(spark)\n",
    "loader.load_reconciliation_batch(batch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract transactions\n",
    "extractor = TransactionExtractor(spark)\n",
    "transactions_by_source = extractor.extract_transactions_for_reconciliation(\n",
    "    source_systems, start_date, end_date\n",
    ")\n",
    "\n",
    "# Print transaction counts\n",
    "for source, df in transactions_by_source.items():\n",
    "    print(f\"{source}: {df.count()} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform transactions\n",
    "transformer = TransactionTransformer(spark)\n",
    "prepared_transactions = transformer.prepare_for_reconciliation(transactions_by_source)\n",
    "\n",
    "# Get primary and secondary DataFrames\n",
    "primary_source = source_systems[0]\n",
    "secondary_source = source_systems[1]\n",
    "primary_df = prepared_transactions[primary_source]\n",
    "secondary_df = prepared_transactions[secondary_source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match transactions\n",
    "matcher = TransactionMatcher(spark)\n",
    "matched_df, unmatched_primary_df, unmatched_secondary_df = matcher.match_transactions(\n",
    "    primary_df, secondary_df, match_strategy=\"hybrid\"\n",
    ")\n",
    "\n",
    "# Print matching results\n",
    "print(f\"Matched: {matched_df.count()} transactions\")\n",
    "print(f\"Unmatched in {primary_source}: {unmatched_primary_df.count()} transactions\")\n",
    "print(f\"Unmatched in {secondary_source}: {unmatched_secondary_df.count()} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reconciliation results\n",
    "results_df = matcher.create_reconciliation_results(\n",
    "    batch_id,\n",
    "    matched_df,\n",
    "    unmatched_primary_df,\n",
    "    unmatched_secondary_df,\n",
    "    primary_source,\n",
    "    secondary_source\n",
    ")\n",
    "\n",
    "# Save reconciliation results\n",
    "loader.load_reconciliation_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reports\n",
    "reporter = ReconciliationReporter(spark)\n",
    "summary_report = reporter.generate_summary_report(results_df)\n",
    "discrepancy_report = reporter.generate_discrepancy_report(results_df)\n",
    "\n",
    "# Display summary report\n",
    "summary_report.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display discrepancy report (first 10 rows)\n",
    "discrepancy_report.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update batch status\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE local.banking.reconciliation_batches\n",
    "    SET \n",
    "        status = 'COMPLETED',\n",
    "        matched_count = {matched_df.count()},\n",
    "        unmatched_count = {unmatched_primary_df.count() + unmatched_secondary_df.count()},\n",
    "        total_transactions = {matched_df.count() + unmatched_primary_df.count() + unmatched_secondary_df.count()},\n",
    "        completed_at = CURRENT_TIMESTAMP()\n",
    "    WHERE batch_id = '{batch_id}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demonstrate ACID Transactions\n",
    "\n",
    "Let's demonstrate ACID transactions by performing a multi-statement transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a transaction\n",
    "spark.sql(\"START TRANSACTION\")\n",
    "\n",
    "try:\n",
    "    # Update some transactions\n",
    "    spark.sql(\"\"\"\n",
    "        UPDATE local.banking.source_transactions\n",
    "        SET status = 'completed'\n",
    "        WHERE status = 'pending' AND source_system = 'core_banking'\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insert a new reconciliation batch\n",
    "    new_batch_id = f\"ACID-{uuid.uuid4().hex[:8]}\"\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO local.banking.reconciliation_batches VALUES (\n",
    "            '{new_batch_id}',\n",
    "            CURRENT_TIMESTAMP(),\n",
    "            ARRAY('core_banking', 'payment_gateway'),\n",
    "            TIMESTAMP('{start_date}'),\n",
    "            TIMESTAMP('{end_date}'),\n",
    "            'PENDING',\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            CURRENT_TIMESTAMP(),\n",
    "            NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Commit the transaction\n",
    "    spark.sql(\"COMMIT\")\n",
    "    print(\"Transaction committed successfully\")\n",
    "except Exception as e:\n",
    "    # Rollback the transaction on error\n",
    "    spark.sql(\"ROLLBACK\")\n",
    "    print(f\"Transaction rolled back due to error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the changes\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM local.banking.reconciliation_batches\n",
    "    WHERE batch_id = '{new_batch_id}'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demonstrate Incremental Processing\n",
    "\n",
    "Let's demonstrate incremental processing by processing only new transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest snapshot timestamp\n",
    "latest_snapshot = spark.sql(\"\"\"\n",
    "    SELECT * FROM local.banking.source_transactions.snapshots\n",
    "    ORDER BY committed_at DESC\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "latest_timestamp = latest_snapshot[\"committed_at\"]\n",
    "print(f\"Latest snapshot timestamp: {latest_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some new transactions\n",
    "import pandas as pd\n",
    "import random\n",
    "from decimal import Decimal\n",
    "\n",
    "# Generate 10 new transactions\n",
    "new_transactions = []\n",
    "for i in range(10):\n",
    "    tx_id = f\"NEW-{uuid.uuid4().hex[:8]}\"\n",
    "    source_system = \"core_banking\"\n",
    "    tx_date = datetime.now()\n",
    "    amount = Decimal(random.uniform(100, 1000)).quantize(Decimal('0.01'))\n",
    "    account_id = f\"ACC{random.randint(10000000, 99999999)}\"\n",
    "    tx_type = random.choice([\"deposit\", \"withdrawal\", \"transfer\", \"payment\"])\n",
    "    ref_id = f\"REF-{random.randint(1000000000, 9999999999)}\"\n",
    "    status = random.choice([\"completed\", \"pending\"])\n",
    "    \n",
    "    new_transactions.append({\n",
    "        \"transaction_id\": tx_id,\n",
    "        \"source_system\": source_system,\n",
    "        \"transaction_date\": tx_date,\n",
    "        \"amount\": amount,\n",
    "        \"account_id\": account_id,\n",
    "        \"transaction_type\": tx_type,\n",
    "        \"reference_id\": ref_id,\n",
    "        \"status\": status,\n",
    "        \"payload\": \"{}\",\n",
    "        \"created_at\": tx_date,\n",
    "        \"processing_timestamp\": tx_date,\n",
    "        \"transaction_category\": \"NEW\"\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the new transactions\n",
    "new_tx_df = spark.createDataFrame(new_transactions)\n",
    "new_tx_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new transactions incrementally\n",
    "loader = IcebergLoader(spark)\n",
    "loader.load_transactions_incrementally(\n",
    "    new_tx_df, \n",
    "    \"core_banking\",\n",
    "    snapshot_time=datetime.now()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query only the new transactions added since the last snapshot\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM local.banking.source_transactions\n",
    "    WHERE _commit_time > TIMESTAMP('{latest_timestamp}')\n",
    "    ORDER BY transaction_date DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the key features of the Apache Iceberg Banking Reconciliation System:\n",
    "\n",
    "1. **Schema Evolution**: Adding new columns without rebuilding tables\n",
    "2. **Time Travel**: Querying historical states of the data\n",
    "3. **Partition Evolution**: Changing partition specifications\n",
    "4. **ACID Transactions**: Ensuring consistency during updates\n",
    "5. **Incremental Processing**: Processing only new data\n",
    "6. **Reconciliation Process**: Matching transactions across systems\n",
    "\n",
    "These features make Apache Iceberg an excellent choice for banking reconciliation systems, providing the reliability, flexibility, and performance needed for financial data processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
