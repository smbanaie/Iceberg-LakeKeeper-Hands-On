{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pyjwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration:\n",
      "   - Catalog URL: http://lakekeeper:8181/catalog\n",
      "   - Management URL: http://lakekeeper:8181/management\n",
      "   - Warehouse: irisa-ot\n",
      "   - Namespace: irisa\n",
      "   - Table: fake_seclink\n"
     ]
    }
   ],
   "source": [
    "import requests, jwt\n",
    "from IPython.display import JSON\n",
    "# from pyiceberg.catalog.rest import RestCatalog\n",
    "import pandas as pd\n",
    "# from pyiceberg.schema import Schema\n",
    "# from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "# from pyiceberg.types import NestedField, StringType, IntegerType, TimestampType\n",
    "# from pyiceberg.transforms import MonthTransform\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import pyarrow as pa\n",
    "import duckdb\n",
    "\n",
    "# Authentication and API endpoints\n",
    "CATALOG_URL = \"http://lakekeeper:8181/catalog\"\n",
    "MANAGEMENT_URL = \"http://lakekeeper:8181/management\"\n",
    "KEYCLOAK_TOKEN_URL = \"http://keycloak:8080/realms/iceberg/protocol/openid-connect/token\"\n",
    "\n",
    "# Table configuration\n",
    "WAREHOUSE = \"irisa-ot\"\n",
    "NAMESPACE = \"irisa\"\n",
    "TABLE_NAME = \"fake_seclink\"\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"   - Catalog URL: {CATALOG_URL}\")\n",
    "print(f\"   - Management URL: {MANAGEMENT_URL}\")\n",
    "print(f\"   - Warehouse: {WAREHOUSE}\")\n",
    "print(f\"   - Namespace: {NAMESPACE}\")\n",
    "print(f\"   - Table: {TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Authenticating with Keycloak...\n",
      "‚úÖ Authentication successful\n",
      "   - Client: service-account-spark\n",
      "   - Expires: 2025-08-04 15:29:40\n"
     ]
    }
   ],
   "source": [
    "# Login to Keycloak for authentication\n",
    "CLIENT_ID = \"spark\"\n",
    "CLIENT_SECRET = \"2OR3eRvYfSZzzZ16MlPd95jhLnOaLM52\"\n",
    "\n",
    "print(\"üîê Authenticating with Keycloak...\")\n",
    "\n",
    "response = requests.post(\n",
    "    url=KEYCLOAK_TOKEN_URL,\n",
    "    data={\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": CLIENT_ID,\n",
    "        \"client_secret\": CLIENT_SECRET,\n",
    "        \"scope\": \"lakekeeper\"\n",
    "    },\n",
    "    headers={\"Content-type\": \"application/x-www-form-urlencoded\"},\n",
    ")\n",
    "response.raise_for_status()\n",
    "access_token = response.json()['access_token']\n",
    "\n",
    "# Verify the token\n",
    "token_data = jwt.decode(access_token, options={\"verify_signature\": False})\n",
    "print(f\"‚úÖ Authentication successful\")\n",
    "print(f\"   - Client: {token_data.get('preferred_username', 'Unknown')}\")\n",
    "print(f\"   - Expires: {datetime.fromtimestamp(token_data.get('exp', 0))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Setting up DuckDB connection with authentication...\n",
      "üì° Step 1: Initializing DuckDB connection...\n",
      "‚úÖ DuckDB connection established\n",
      "\n",
      "üìÇ Step 2: Loading DuckDB extensions...\n",
      "\n",
      "üîç Step 3: Verifying extensions...\n",
      "   üîç httpfs extension verified\n",
      "   üîç iceberg extension verified\n",
      "\n",
      "üîó Step 4: Attaching Iceberg catalog with warehouse: irisa-ot\n",
      "   üîó Attempting to attach Iceberg catalog...\n",
      "   üì° Catalog URL: http://lakekeeper:8181/catalog\n",
      "   >> Warehouse: irisa-ot\n",
      "   üîê Using authenticated token: eyJhbGciOiJSUzI1NiIs...\n",
      "   ‚úÖ Iceberg catalog attached successfully\n",
      "   üîç Verifying catalog attachment...\n",
      "   ‚úÖ Catalog verification successful - found 1 test records\n",
      "\n",
      "‚úÖ DuckDB setup completed!\n",
      ">> Summary:\n",
      "   - Extensions installed: ['httpfs', 'iceberg']\n",
      "   - Authentication: Using Keycloak token\n",
      "   - Warehouse: irisa-ot\n",
      "\n",
      "üîç Debug Information:\n",
      "   - DuckDB version: v1.3.2\n",
      "   - Working directory: /opt/jupyter/notebooks\n",
      "   - Environment: Containerized (no internet access detected)\n"
     ]
    }
   ],
   "source": [
    "# Connect to DuckDB with authentication context and robust extension handling\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "print(\"üîó Setting up DuckDB connection with authentication...\")\n",
    "\n",
    "# Initialize DuckDB connection\n",
    "print(\"üì° Step 1: Initializing DuckDB connection...\")\n",
    "con = duckdb.connect(\"local.duckdb\")\n",
    "# con = duckdb.connect()\n",
    "print(\"‚úÖ DuckDB connection established\")\n",
    "\n",
    "con.sql(\"INSTALL ICEBERG;\");\n",
    "con.sql(\"INSTALL httpfs;\");\n",
    "# con.sql(\"UPDATE EXTENSIONS;\");\n",
    "con.sql(\"LOAD ICEBERG;\");\n",
    "con.sql(\"LOAD httpfs;\");\n",
    "\n",
    "# Load extensions separately with detailed logging\n",
    "print(\"\\nüìÇ Step 2: Loading DuckDB extensions...\")\n",
    "\n",
    "installed_extensions = []\n",
    "\n",
    "# Verify extensions are working\n",
    "print(\"\\nüîç Step 3: Verifying extensions...\")\n",
    "result = con.sql(\"SELECT * FROM duckdb_extensions()\").fetchall()\n",
    "for ext in result:\n",
    "    try:\n",
    "        if ext[0] in ['iceberg', 'httpfs']:\n",
    "            print(f\"   üîç {ext[0]} extension verified\")\n",
    "            installed_extensions.append(ext[0])\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed to verify {ext}: {e}\")\n",
    "\n",
    "# Step 5: Attach the Iceberg catalog with authentication\n",
    "print(f\"\\nüîó Step 4: Attaching Iceberg catalog with warehouse: {WAREHOUSE}\")\n",
    "\n",
    "if \"iceberg\" in installed_extensions:\n",
    "    try:\n",
    "        print(\"   üîó Attempting to attach Iceberg catalog...\")\n",
    "        print(f\"   üì° Catalog URL: {CATALOG_URL}\")\n",
    "        print(f\"   >> Warehouse: {WAREHOUSE}\")\n",
    "        print(f\"   üîê Using authenticated token: {access_token[:20]}...\")\n",
    "        \n",
    "        try : \n",
    "            # First create the secret\n",
    "            con.sql(f\"\"\"\n",
    "                CREATE SECRET lakekeeper_secret (\n",
    "                    TYPE ICEBERG,\n",
    "                    CLIENT_ID '{CLIENT_ID}',\n",
    "                    CLIENT_SECRET '{CLIENT_SECRET}',\n",
    "                    OAUTH2_SCOPE 'lakekeeper',\n",
    "                    OAUTH2_SERVER_URI '{KEYCLOAK_TOKEN_URL}'\n",
    "                )\n",
    "            \"\"\")\n",
    "        except:\n",
    "            print(f\" ‚ö†Ô∏è Secret `lakekeeper_secret` already Exists!\")\n",
    "        try:\n",
    "            # Then attach using the secret\n",
    "            con.sql(f\"\"\"\n",
    "                ATTACH '{WAREHOUSE}' AS irisa_datalake (\n",
    "                    TYPE ICEBERG,\n",
    "                    ENDPOINT '{CATALOG_URL}',\n",
    "                    SECRET lakekeeper_secret\n",
    "                )\n",
    "            \"\"\")\n",
    "        except:\n",
    "            print(f\" ‚ö†Ô∏è Warehouse `irisa_datalake` already Exists!\")\n",
    "        \n",
    "        print(\"   ‚úÖ Iceberg catalog attached successfully\")\n",
    "        \n",
    "        # Verify the attachment by checking if we can query the catalog\n",
    "        print(\"   üîç Verifying catalog attachment...\")\n",
    "        try:\n",
    "            # Try to list tables in the catalog to verify it's working\n",
    "            tables = con.sql(f\"SELECT * FROM irisa_datalake.irisa.fake_seclink LIMIT 1\").fetchall()\n",
    "            print(f\"   ‚úÖ Catalog verification successful - found {len(tables)} test records\")\n",
    "        except Exception as verify_error:\n",
    "            print(f\"   ‚ö†Ô∏è Catalog verification failed: {verify_error}\")\n",
    "            print(\"   ‚ÑπÔ∏è This might be normal if the table doesn't exist yet\")\n",
    "                \n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed to attach Iceberg catalog: {e}\")\n",
    "        print(\"   üîß This might be due to network issues or authentication problems\")\n",
    "        raise Exception(f\"Failed to attach Iceberg catalog: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Skipping Iceberg catalog attachment - iceberg extension not loaded\")\n",
    "\n",
    "print(\"\\n‚úÖ DuckDB setup completed!\")\n",
    "print(f\">> Summary:\")\n",
    "print(f\"   - Extensions installed: {installed_extensions}\")\n",
    "print(f\"   - Authentication: Using Keycloak token\")\n",
    "print(f\"   - Warehouse: {WAREHOUSE}\")\n",
    "\n",
    "# Additional debugging information\n",
    "print(f\"\\nüîç Debug Information:\")\n",
    "print(f\"   - DuckDB version: {con.sql('SELECT version()').fetchone()[0]}\")\n",
    "print(f\"   - Working directory: {os.getcwd()}\")\n",
    "print(f\"   - Environment: Containerized (no internet access detected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Running queries to test the data and partitioning...\n",
      "   - Authentication: Using Keycloak token\n",
      "   - Warehouse: irisa-ot\n",
      "\n",
      "üìä Query 1: Total record count\n",
      "Total records: 10000\n",
      "\n",
      "üìÖ Query 2: Records by month (partitioning test)\n",
      "Month 1 (2024-01): 1718 records\n",
      "Month 2 (2024-02): 1606 records\n",
      "Month 3 (2024-03): 1753 records\n",
      "Month 4 (2024-04): 1585 records\n",
      "Month 5 (2024-05): 1652 records\n",
      "Month 6 (2024-06): 1686 records\n",
      "\n",
      "üè¢ Query 3: Top sources by record count\n",
      "Source 2: 2024 records\n",
      "Source 5: 2008 records\n",
      "Source 4: 1998 records\n",
      "Source 1: 1988 records\n",
      "Source 3: 1982 records\n",
      "\n",
      "‚è±Ô∏è Query 4: Average processing time analysis\n",
      "Average processing time: 30.50 minutes\n",
      "Min processing time: 1.00 minutes\n",
      "Max processing time: 60.00 minutes\n",
      "\n",
      "üïê Query 5: Busiest hour of the day\n",
      "Busiest hour: 11:00 with 447 records\n",
      "\n",
      ">> Query 6: Sample of recent records\n",
      "  Id              DateIn  Source  Destination  ProcessingTime\n",
      "7897 2024-06-30 23:17:35       3            4            35.0\n",
      "4609 2024-06-30 22:56:02       4            5            25.0\n",
      "1410 2024-06-30 22:48:10       2            2            46.0\n",
      "7564 2024-06-30 22:43:24       3            5             7.0\n",
      "  31 2024-06-30 22:41:44       2            2            34.0\n",
      "4583 2024-06-30 22:21:03       1            4            13.0\n",
      "3303 2024-06-30 22:19:48       5            1            12.0\n",
      "7898 2024-06-30 21:27:59       5            3            55.0\n",
      " 488 2024-06-30 21:11:15       1            5             8.0\n",
      "1483 2024-06-30 20:47:35       1            2             6.0\n",
      "\n",
      "üìà Query 7: Performance analysis by source system\n",
      " Source  avg_min  min_min  max_min  count\n",
      "      2    30.39      1.0     60.0   2024\n",
      "      5    30.26      1.0     60.0   2008\n",
      "      4    30.41      1.0     60.0   1998\n",
      "      1    30.27      1.0     60.0   1988\n",
      "      3    31.17      1.0     60.0   1982\n",
      "\n",
      "‚úÖ All queries completed successfully!\n",
      "üéØ The month partitioning is working efficiently for time-based queries!\n",
      "üîê All operations performed with proper Keycloak authentication\n"
     ]
    }
   ],
   "source": [
    "print(\">> Running queries to test the data and partitioning...\")\n",
    "print(\"   - Authentication: Using Keycloak token\")\n",
    "print(\"   - Warehouse: \" + WAREHOUSE)\n",
    "\n",
    "# Iceberg table reference\n",
    "table_ref = \"irisa_datalake.irisa.fake_seclink\"\n",
    "\n",
    "# Query 1: Total record count\n",
    "print(\"\\nüìä Query 1: Total record count\")\n",
    "total_count = con.sql(f\"SELECT COUNT(*) FROM {table_ref}\").fetchone()[0]\n",
    "print(f\"Total records: {total_count}\")\n",
    "\n",
    "# Query 2: Records by month (demonstrating partitioning benefits)\n",
    "print(\"\\nüìÖ Query 2: Records by month (partitioning test)\")\n",
    "for month in range(1, 7):\n",
    "    result = con.sql(f\"\"\"\n",
    "        SELECT COUNT(*) FROM {table_ref}\n",
    "        WHERE EXTRACT(MONTH FROM DateIn) = {month} AND EXTRACT(YEAR FROM DateIn) = 2024\n",
    "    \"\"\").fetchone()[0]\n",
    "    print(f\"Month {month} (2024-{month:02d}): {result} records\")\n",
    "\n",
    "# Query 3: Top sources by record count\n",
    "print(\"\\nüè¢ Query 3: Top sources by record count\")\n",
    "top_sources = con.sql(f\"\"\"\n",
    "    SELECT Source, COUNT(*) as count\n",
    "    FROM {table_ref}\n",
    "    GROUP BY Source\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\").fetchall()\n",
    "for source, count in top_sources:\n",
    "    print(f\"Source {source}: {count} records\")\n",
    "\n",
    "# Query 4: Average processing time analysis\n",
    "print(\"\\n‚è±Ô∏è Query 4: Average processing time analysis\")\n",
    "avg_stats = con.sql(f\"\"\"\n",
    "    SELECT \n",
    "        AVG(EXTRACT(EPOCH FROM DateOut - DateIn) / 60) as avg_min,\n",
    "        MIN(EXTRACT(EPOCH FROM DateOut - DateIn) / 60) as min_min,\n",
    "        MAX(EXTRACT(EPOCH FROM DateOut - DateIn) / 60) as max_min\n",
    "    FROM {table_ref}\n",
    "    WHERE DateOut IS NOT NULL AND DateIn IS NOT NULL\n",
    "\"\"\").fetchone()\n",
    "print(f\"Average processing time: {avg_stats[0]:.2f} minutes\")\n",
    "print(f\"Min processing time: {avg_stats[1]:.2f} minutes\")\n",
    "print(f\"Max processing time: {avg_stats[2]:.2f} minutes\")\n",
    "\n",
    "# Query 5: Busiest hour of the day\n",
    "print(\"\\nüïê Query 5: Busiest hour of the day\")\n",
    "hour_stats = con.sql(f\"\"\"\n",
    "    SELECT EXTRACT(HOUR FROM DateIn) as hour, COUNT(*) as count\n",
    "    FROM {table_ref}\n",
    "    GROUP BY hour\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 1\n",
    "\"\"\").fetchone()\n",
    "print(f\"Busiest hour: {int(hour_stats[0])}:00 with {hour_stats[1]} records\")\n",
    "\n",
    "# Query 6: Sample of recent records\n",
    "print(\"\\n>> Query 6: Sample of recent records\")\n",
    "recent_records = con.sql(f\"\"\"\n",
    "    SELECT Id, DateIn, Source, Destination, \n",
    "           ROUND(EXTRACT(EPOCH FROM DateOut - DateIn) / 60, 2) AS ProcessingTime\n",
    "    FROM {table_ref}\n",
    "    WHERE DateOut IS NOT NULL AND DateIn IS NOT NULL\n",
    "    ORDER BY DateIn DESC\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "print(recent_records.to_string(index=False))\n",
    "\n",
    "# Query 7: Performance by source system\n",
    "print(\"\\nüìà Query 7: Performance analysis by source system\")\n",
    "performance_stats = con.sql(f\"\"\"\n",
    "    SELECT \n",
    "        Source,\n",
    "        ROUND(AVG(EXTRACT(EPOCH FROM DateOut - DateIn) / 60), 2) as avg_min,\n",
    "        ROUND(MIN(EXTRACT(EPOCH FROM DateOut - DateIn) / 60), 2) as min_min,\n",
    "        ROUND(MAX(EXTRACT(EPOCH FROM DateOut - DateIn) / 60), 2) as max_min,\n",
    "        COUNT(*) as count\n",
    "    FROM {table_ref}\n",
    "    WHERE DateOut IS NOT NULL AND DateIn IS NOT NULL\n",
    "    GROUP BY Source\n",
    "    ORDER BY count DESC\n",
    "\"\"\").df()\n",
    "print(performance_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ All queries completed successfully!\")\n",
    "print(\"üéØ The month partitioning is working efficiently for time-based queries!\")\n",
    "print(\"üîê All operations performed with proper Keycloak authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
