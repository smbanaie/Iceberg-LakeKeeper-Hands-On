{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Fake SecLink Table Setup (Timestamp Fixed)\n",
    "## Apache Iceberg with Lakekeeper Catalog and MinIO Storage\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "Create a minimal Apache Iceberg table for `fake_seclink` using **PyIceberg** with **Lakekeeper** catalog and **MinIO** storage.\n",
    "\n",
    "### **Table Schema**\n",
    "```sql\n",
    "CREATE TABLE fake_seclink (\n",
    "    Id INT,\n",
    "    TelegramCode INT,\n",
    "    Source INT,\n",
    "    Destination INT,\n",
    "    DateIn TIMESTAMP,\n",
    "    DateOut TIMESTAMP,\n",
    "    Body VARCHAR(max)\n",
    ");\n",
    "```\n",
    "\n",
    "### **Partitioning Strategy**\n",
    "- **Primary Partition**: `month(DateIn)` - Time-based partitioning\n",
    "- **Benefits**: Efficient time-based queries and data organization\n",
    "\n",
    "### **‚ö†Ô∏è Important Fix**\n",
    "- **Timestamp Precision**: Using microsecond precision to avoid Iceberg compatibility issues\n",
    "- **PyArrow Schema**: Explicitly defining schema with `timestamp('us')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration:\n",
      "   - Catalog URL: http://lakekeeper:8181/catalog\n",
      "   - Warehouse: irisa-ot\n",
      "   - Namespace: irisa\n",
      "   - Table: fake_seclink\n"
     ]
    }
   ],
   "source": [
    "from pyiceberg.catalog.rest import RestCatalog\n",
    "import pandas as pd\n",
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.partitioning import PartitionSpec, PartitionField\n",
    "from pyiceberg.types import NestedField, StringType, IntegerType, TimestampType\n",
    "from pyiceberg.transforms import MonthTransform\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import pyarrow as pa\n",
    "\n",
    "# Catalog configuration\n",
    "CATALOG_URL = \"http://lakekeeper:8181/catalog\"\n",
    "WAREHOUSE = \"irisa-ot\"\n",
    "NAMESPACE = \"irisa\"\n",
    "TABLE_NAME = \"fake_seclink\"\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"   - Catalog URL: {CATALOG_URL}\")\n",
    "print(f\"   - Warehouse: {WAREHOUSE}\")\n",
    "print(f\"   - Namespace: {NAMESPACE}\")\n",
    "print(f\"   - Table: {TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Catalog initialized successfully\n",
      "Available namespaces: []\n"
     ]
    }
   ],
   "source": [
    "# Initialize the catalog\n",
    "catalog = RestCatalog(\n",
    "    name=\"irisa_catalog\",\n",
    "    warehouse=WAREHOUSE,\n",
    "    uri=CATALOG_URL,\n",
    "    token=\"dummy\",\n",
    ")\n",
    "\n",
    "print(\"‚úì Catalog initialized successfully\")\n",
    "print(f\"Available namespaces: {list(catalog.list_namespaces())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created namespace: irisa\n",
      "üìã Available namespaces: [('irisa',)]\n"
     ]
    }
   ],
   "source": [
    "# Create the irisa namespace if it doesn't exist\n",
    "irisa_namespace = (NAMESPACE,)\n",
    "\n",
    "if irisa_namespace not in catalog.list_namespaces():\n",
    "    catalog.create_namespace(irisa_namespace)\n",
    "    print(f\"‚úì Created namespace: {NAMESPACE}\")\n",
    "else:\n",
    "    print(f\"‚Ñπ Namespace '{NAMESPACE}' already exists\")\n",
    "\n",
    "print(f\"üìã Available namespaces: {list(catalog.list_namespaces())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Schema Defined:\n",
      "   - Total fields: 7\n",
      "   - Id: int\n",
      "   - TelegramCode: int\n",
      "   - Source: int\n",
      "   - Destination: int\n",
      "   - DateIn: timestamp\n",
      "   - DateOut: timestamp\n",
      "   - Body: string\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for fake_seclink using PyIceberg (FIXED: using TimestampType for dates)\n",
    "schema = Schema(\n",
    "    NestedField(field_id=1, name=\"Id\", field_type=IntegerType(), required=True),\n",
    "    NestedField(field_id=2, name=\"TelegramCode\", field_type=IntegerType(), required=False),\n",
    "    NestedField(field_id=3, name=\"Source\", field_type=IntegerType(), required=False),\n",
    "    NestedField(field_id=4, name=\"Destination\", field_type=IntegerType(), required=False),\n",
    "    NestedField(field_id=5, name=\"DateIn\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=6, name=\"DateOut\", field_type=TimestampType(), required=False),\n",
    "    NestedField(field_id=7, name=\"Body\", field_type=StringType(), required=False),\n",
    ")\n",
    "\n",
    "print(\"üìã Schema Defined:\")\n",
    "print(f\"   - Total fields: {len(schema.fields)}\")\n",
    "for field in schema.fields:\n",
    "   print(f\"   - {field.name}: {field.field_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Partitioning Strategy:\n",
      "   - Partition by: month(DateIn)\n",
      "   - Total partition fields: 1\n"
     ]
    }
   ],
   "source": [
    "# Define partitioning strategy (by month of DateIn)\n",
    "partition_spec = PartitionSpec(\n",
    "    PartitionField(\n",
    "        source_id=5,  # DateIn field ID (matches the field ID in schema definition)\n",
    "        field_id=1000,\n",
    "        name=\"DateIn_month\",\n",
    "        transform=MonthTransform()\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"üîß Partitioning Strategy:\")\n",
    "print(f\"   - Partition by: month(DateIn)\")\n",
    "print(f\"   - Total partition fields: {len(partition_spec.fields)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Table created successfully!\n",
      "   - Table: irisa.fake_seclink\n",
      "   - Location: s3://irisa-warehouse/ot/0198503b-e3f3-7643-b7a1-e4fff6a0230e/0198503b-e46a-7530-9673-d495a364eaf0\n",
      "   - Format: 2\n",
      "   - Partitioning: 1 fields\n"
     ]
    }
   ],
   "source": [
    "# Create the table\n",
    "table_identifier = (NAMESPACE, TABLE_NAME)\n",
    "\n",
    "# Check if table already exists\n",
    "if table_identifier in catalog.list_tables(namespace=irisa_namespace):\n",
    "    print(f\"‚ö† Table '{TABLE_NAME}' already exists in namespace '{NAMESPACE}'\")\n",
    "    print(\"   Dropping existing table...\")\n",
    "    catalog.drop_table(table_identifier)\n",
    "    print(\"   ‚úì Existing table dropped\")\n",
    "\n",
    "# Create the new table\n",
    "try:\n",
    "    table = catalog.create_table(\n",
    "        identifier=table_identifier,\n",
    "        schema=schema,\n",
    "        partition_spec=partition_spec\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Table created successfully!\")\n",
    "    print(f\"   - Table: {NAMESPACE}.{TABLE_NAME}\")\n",
    "    print(f\"   - Location: {table.location()}\")\n",
    "    print(f\"   - Format: {table.format_version}\")\n",
    "    print(f\"   - Partitioning: {len(partition_spec.fields)} fields\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating table: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Generating fake data...\n",
      "üîß Converting timestamps to microsecond precision...\n",
      "‚úÖ Timestamps converted to microsecond precision\n",
      "‚úÖ Generated 10000 fake records\n",
      "üìÖ Date range: 2024-01-01 to 2024-06-30\n",
      "üìä Sample data:\n",
      "   Id  TelegramCode  Source  Destination              DateIn  \\\n",
      "0   1          3296       3            1 2024-03-17 11:12:34   \n",
      "1   2          5351       2            4 2024-03-18 08:41:47   \n",
      "2   3          1420       5            1 2024-02-09 10:46:30   \n",
      "3   4          7588       5            3 2024-03-12 21:54:06   \n",
      "4   5          7405       1            4 2024-05-15 15:03:25   \n",
      "\n",
      "              DateOut                                               Body  \n",
      "0 2024-03-17 11:31:34  Message body for record 1 from source 1 to des...  \n",
      "1 2024-03-18 09:09:47  Message body for record 2 from source 5 to des...  \n",
      "2 2024-02-09 11:00:30  Message body for record 3 from source 2 to des...  \n",
      "3 2024-03-12 22:07:06  Message body for record 4 from source 4 to des...  \n",
      "4 2024-05-15 15:08:25  Message body for record 5 from source 1 to des...  \n",
      "\n",
      "üìà Monthly distribution:\n",
      "   - 2024-01: 1684 records\n",
      "   - 2024-02: 1593 records\n",
      "   - 2024-03: 1717 records\n",
      "   - 2024-04: 1612 records\n",
      "   - 2024-05: 1703 records\n",
      "   - 2024-06: 1691 records\n"
     ]
    }
   ],
   "source": [
    "# Generate fake data for 6 months (10,000 records)\n",
    "print(\"üé≤ Generating fake data...\")\n",
    "\n",
    "# Set up date range for 6 months\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2024, 6, 30)\n",
    "total_records = 10000\n",
    "\n",
    "# Generate random data\n",
    "data = []\n",
    "for i in range(total_records):\n",
    "    # Random date within the 6-month period\n",
    "    random_days = random.randint(0, (end_date - start_date).days)\n",
    "    date_in = start_date + timedelta(days=random_days)\n",
    "    \n",
    "    # Random time within the day\n",
    "    random_hours = random.randint(0, 23)\n",
    "    random_minutes = random.randint(0, 59)\n",
    "    random_seconds = random.randint(0, 59)\n",
    "    date_in = date_in.replace(hour=random_hours, minute=random_minutes, second=random_seconds)\n",
    "    \n",
    "    # DateOut is typically 1-60 minutes after DateIn\n",
    "    random_duration = random.randint(1, 60)\n",
    "    date_out = date_in + timedelta(minutes=random_duration)\n",
    "    \n",
    "    record = {\n",
    "        \"Id\": i + 1,\n",
    "        \"TelegramCode\": random.randint(1000, 9999),\n",
    "        \"Source\": random.randint(1, 5),\n",
    "        \"Destination\": random.randint(1, 5),\n",
    "        \"DateIn\": date_in,  # Using datetime objects directly\n",
    "        \"DateOut\": date_out,  # Using datetime objects directly\n",
    "        \"Body\": f\"Message body for record {i + 1} from source {random.randint(1, 5)} to destination {random.randint(1, 5)}\"\n",
    "    }\n",
    "    data.append(record)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# üîß CRITICAL FIX: Convert timestamps to microsecond precision\n",
    "print(\"üîß Converting timestamps to microsecond precision...\")\n",
    "df['DateIn'] = df['DateIn'].dt.floor('us')  # Round down to microsecond precision\n",
    "df['DateOut'] = df['DateOut'].dt.floor('us')  # Round down to microsecond precision\n",
    "print(\"‚úÖ Timestamps converted to microsecond precision\")\n",
    "\n",
    "print(f\"‚úÖ Generated {len(df)} fake records\")\n",
    "print(f\"üìÖ Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"üìä Sample data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Show distribution by month\n",
    "monthly_distribution = df['DateIn'].dt.to_period('M').value_counts().sort_index()\n",
    "print(f\"\\nüìà Monthly distribution:\")\n",
    "for month, count in monthly_distribution.items():\n",
    "    print(f\"   - {month}: {count} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Inserting data into Iceberg table...\n",
      "‚úÖ PyArrow table created with microsecond precision timestamps\n",
      "‚úÖ Successfully inserted 10000 records into irisa.fake_seclink\n",
      "\n",
      "üîç Data insertion completed successfully!\n",
      "üìä Expected records: 10000\n",
      "‚úÖ Table: irisa.fake_seclink is ready for queries!\n"
     ]
    }
   ],
   "source": [
    "# Insert data into the Iceberg table (FIXED: with explicit PyArrow schema)\n",
    "print(\"üì§ Inserting data into Iceberg table...\")\n",
    "\n",
    "try:\n",
    "    # Load the table\n",
    "    table = catalog.load_table(table_identifier)\n",
    "    \n",
    "    # üîß CRITICAL FIX: Convert DataFrame to PyArrow table with explicit microsecond precision schema\n",
    "    # Note: Making Id field required to match the table schema\n",
    "    arrow_table = pa.Table.from_pandas(df, schema=pa.schema([\n",
    "        pa.field(\"Id\", pa.int32(), nullable=False),  # Required field\n",
    "        pa.field(\"TelegramCode\", pa.int32(), nullable=True),\n",
    "        pa.field(\"Source\", pa.int32(), nullable=True),\n",
    "        pa.field(\"Destination\", pa.int32(), nullable=True),\n",
    "        pa.field(\"DateIn\", pa.timestamp('us'), nullable=True),  # Microsecond precision\n",
    "        pa.field(\"DateOut\", pa.timestamp('us'), nullable=True),  # Microsecond precision\n",
    "        pa.field(\"Body\", pa.string(), nullable=True),\n",
    "    ]))\n",
    "    \n",
    "    print(\"‚úÖ PyArrow table created with microsecond precision timestamps\")\n",
    "    \n",
    "    # Append data to the table\n",
    "    table.append(arrow_table)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully inserted {len(df)} records into {NAMESPACE}.{TABLE_NAME}\")\n",
    "    \n",
    "    # Simple verification\n",
    "    print(f\"\\nüîç Data insertion completed successfully!\")\n",
    "    print(f\"üìä Expected records: {len(df)}\")\n",
    "    print(f\"‚úÖ Table: {NAMESPACE}.{TABLE_NAME} is ready for queries!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error inserting data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6233f4206e4d5da40516aa998bcbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772480ea2d8f43ba8922b2a0780486c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# ‚úÖ Connect to a persistent DuckDB file\n",
    "con = duckdb.connect(\"local.duckdb\")\n",
    "\n",
    "# ‚úÖ Install extensions (safe even if already installed)\n",
    "for ext in [\"iceberg\", \"httpfs\"]:\n",
    "    try:\n",
    "        con.install_extension(ext)\n",
    "    except duckdb.IOException as e:\n",
    "        print(f\"‚ö†Ô∏è Could not install {ext}: {e}\")\n",
    "\n",
    "# ‚úÖ Load extensions\n",
    "for ext in [\"iceberg\", \"httpfs\"]:\n",
    "    try:\n",
    "        con.load_extension(ext)\n",
    "    except duckdb.IOException as e:\n",
    "        print(f\"‚ùå Could not load {ext}: {e}\")\n",
    "\n",
    "# ‚úÖ Attach the Iceberg catalog\n",
    "CATALOG_URL = \"http://lakekeeper:8181/catalog\"\n",
    "WAREHOUSE = \"irisa-ot\"\n",
    "\n",
    "con.sql(f\"\"\"\n",
    "    ATTACH '{WAREHOUSE}' AS irisa_datalake (\n",
    "        TYPE ICEBERG,\n",
    "        ENDPOINT '{CATALOG_URL}',\n",
    "        TOKEN ''\n",
    "    )\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running queries to test the data and partitioning...\n",
      "\n",
      "üìä Query 1: Total record count\n",
      "Total records: 10000\n",
      "\n",
      "üìÖ Query 2: Records by month (partitioning test)\n",
      "Month 1 (2024-01): 1684 records\n",
      "Month 2 (2024-02): 1593 records\n",
      "Month 3 (2024-03): 1717 records\n",
      "Month 4 (2024-04): 1612 records\n",
      "Month 5 (2024-05): 1703 records\n",
      "Month 6 (2024-06): 1691 records\n",
      "\n",
      "üè¢ Query 3: Top sources by record count\n",
      "Source 3: 2062 records\n",
      "Source 2: 2042 records\n",
      "Source 4: 2003 records\n",
      "Source 1: 1979 records\n",
      "Source 5: 1914 records\n",
      "\n",
      "‚è±Ô∏è Query 4: Average processing time analysis\n",
      "Average processing time: 30.38 minutes\n",
      "Min processing time: 1.00 minutes\n",
      "Max processing time: 60.00 minutes\n",
      "\n",
      "üïê Query 5: Busiest hour of the day\n",
      "Busiest hour: 15:00 with 444 records\n",
      "\n",
      "üìã Query 6: Sample of recent records\n",
      "  Id              DateIn  Source  Destination  ProcessingTime\n",
      "5951 2024-06-30 23:37:25       5            5            12.0\n",
      " 244 2024-06-30 23:23:27       1            4            48.0\n",
      "5383 2024-06-30 23:19:29       1            1            13.0\n",
      "4751 2024-06-30 21:38:51       2            4            23.0\n",
      "5416 2024-06-30 21:04:48       4            3            36.0\n",
      "7250 2024-06-30 21:01:10       3            1             8.0\n",
      " 424 2024-06-30 20:17:41       5            1            48.0\n",
      "9787 2024-06-30 19:54:23       4            1            34.0\n",
      "9674 2024-06-30 19:35:24       4            2             4.0\n",
      "7604 2024-06-30 19:13:21       5            5            22.0\n",
      "\n",
      "üìà Query 7: Performance analysis by source system\n",
      " Source  avg_min  min_min  max_min  count\n",
      "      3    30.50      1.0     60.0   2062\n",
      "      2    30.90      1.0     60.0   2042\n",
      "      4    30.19      1.0     60.0   2003\n",
      "      1    30.35      1.0     60.0   1979\n",
      "      5    29.90      1.0     60.0   1914\n",
      "\n",
      "‚úÖ All queries completed successfully!\n",
      "üéØ The month partitioning is working efficiently for time-based queries!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"üîç Running queries to test the data and partitioning...\")\n",
    "\n",
    "# Iceberg table reference\n",
    "table_ref = \"irisa_datalake.irisa.fake_seclink\"\n",
    "\n",
    "# Query 1: Total record count\n",
    "print(\"\\nüìä Query 1: Total record count\")\n",
    "total_count = con.sql(f\"SELECT COUNT(*) FROM {table_ref}\").fetchone()[0]\n",
    "print(f\"Total records: {total_count}\")\n",
    "\n",
    "# Query 2: Records by month (demonstrating partitioning benefits) - FIXED\n",
    "print(\"\\nüìÖ Query 2: Records by month (partitioning test)\")\n",
    "for month in range(1, 7):\n",
    "    result = con.sql(f\"\"\"\n",
    "        SELECT COUNT(*) FROM {table_ref}\n",
    "        WHERE EXTRACT(MONTH FROM DateIn) = {month} AND EXTRACT(YEAR FROM DateIn) = 2024\n",
    "    \"\"\").fetchone()[0]\n",
    "    print(f\"Month {month} (2024-{month:02d}): {result} records\")\n",
    "    \n",
    "# Query 3: Top sources by record count\n",
    "print(\"\\nüè¢ Query 3: Top sources by record count\")\n",
    "top_sources = con.sql(f\"\"\"\n",
    "    SELECT Source, COUNT(*) as count\n",
    "    FROM {table_ref}\n",
    "    GROUP BY Source\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\").fetchall()\n",
    "for source, count in top_sources:\n",
    "    print(f\"Source {source}: {count} records\")\n",
    "\n",
    "# Query 4: Average processing time (DateOut - DateIn)\n",
    "print(\"\\n‚è±Ô∏è Query 4: Average processing time analysis\")\n",
    "avg_stats = con.sql(f\"\"\"\n",
    "    SELECT \n",
    "        AVG(EXTRACT(EPOCH FROM DateOut - DateIn) / 60) as avg_min,\n",
    "        MIN(EXTRACT(EPOCH FROM DateOut - DateIn) / 60) as min_min,\n",
    "        MAX(EXTRACT(EPOCH FROM DateOut - DateIn) / 60) as max_min\n",
    "    FROM {table_ref}\n",
    "    WHERE DateOut IS NOT NULL AND DateIn IS NOT NULL\n",
    "\"\"\").fetchone()\n",
    "print(f\"Average processing time: {avg_stats[0]:.2f} minutes\")\n",
    "print(f\"Min processing time: {avg_stats[1]:.2f} minutes\")\n",
    "print(f\"Max processing time: {avg_stats[2]:.2f} minutes\")\n",
    "\n",
    "# Query 5: Busiest hour of the day\n",
    "print(\"\\nüïê Query 5: Busiest hour of the day\")\n",
    "hour_stats = con.sql(f\"\"\"\n",
    "    SELECT EXTRACT(HOUR FROM DateIn) as hour, COUNT(*) as count\n",
    "    FROM {table_ref}\n",
    "    GROUP BY hour\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 1\n",
    "\"\"\").fetchone()\n",
    "print(f\"Busiest hour: {int(hour_stats[0])}:00 with {hour_stats[1]} records\")\n",
    "\n",
    "# Query 6: Sample of recent records (last 10)\n",
    "print(\"\\nüìã Query 6: Sample of recent records\")\n",
    "recent_records = con.sql(f\"\"\"\n",
    "    SELECT Id, DateIn, Source, Destination, \n",
    "           ROUND(EXTRACT(EPOCH FROM DateOut - DateIn) / 60, 2) AS ProcessingTime\n",
    "    FROM {table_ref}\n",
    "    WHERE DateOut IS NOT NULL AND DateIn IS NOT NULL\n",
    "    ORDER BY DateIn DESC\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "print(recent_records.to_string(index=False))\n",
    "\n",
    "# Query 7: Performance by source system\n",
    "print(\"\\nüìà Query 7: Performance analysis by source system\")\n",
    "performance_stats = con.sql(f\"\"\"\n",
    "    SELECT \n",
    "        Source,\n",
    "        ROUND(AVG(EXTRACT(EPOCH FROM DateOut - DateIn) / 60), 2) as avg_min,\n",
    "        ROUND(MIN(EXTRACT(EPOCH FROM DateOut - DateIn) / 60), 2) as min_min,\n",
    "        ROUND(MAX(EXTRACT(EPOCH FROM DateOut - DateIn) / 60), 2) as max_min,\n",
    "        COUNT(*) as count\n",
    "    FROM {table_ref}\n",
    "    WHERE DateOut IS NOT NULL AND DateIn IS NOT NULL\n",
    "    GROUP BY Source\n",
    "    ORDER BY count DESC\n",
    "\"\"\").df()\n",
    "print(performance_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ All queries completed successfully!\")\n",
    "print(\"üéØ The month partitioning is working efficiently for time-based queries!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
